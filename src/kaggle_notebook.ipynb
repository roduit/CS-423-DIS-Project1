{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef99507f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-21T12:49:16.632422Z",
     "iopub.status.busy": "2024-10-21T12:49:16.631934Z",
     "iopub.status.idle": "2024-10-21T12:49:16.639135Z",
     "shell.execute_reply": "2024-10-21T12:49:16.637864Z"
    },
    "papermill": {
     "duration": 0.02072,
     "end_time": "2024-10-21T12:49:16.642311",
     "exception": false,
     "start_time": "2024-10-21T12:49:16.621591",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# -*- authors : Vincent Roduit -*-\n",
    "# -*- date : 2024-09-30 -*-\n",
    "# -*- Last revision: 2024-09-30 by Vincent Roduit -*-\n",
    "# -*- python version : 3.9.19 -*-\n",
    "# -*- Description: Constants used in the code *-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e0eb30",
   "metadata": {
    "papermill": {
     "duration": 0.006534,
     "end_time": "2024-10-21T12:49:16.656282",
     "exception": false,
     "start_time": "2024-10-21T12:49:16.649748",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <center> CS - 423: Distributed Information Systems </center>\n",
    "## <center> Ecole Polytechnique Fédérale de Lausanne </center>\n",
    "### <center>Project 1: Document Retrieval </center>\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1caba2a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-21T12:49:16.672275Z",
     "iopub.status.busy": "2024-10-21T12:49:16.671800Z",
     "iopub.status.idle": "2024-10-21T12:49:20.176588Z",
     "shell.execute_reply": "2024-10-21T12:49:20.175176Z"
    },
    "papermill": {
     "duration": 3.516527,
     "end_time": "2024-10-21T12:49:20.179805",
     "exception": false,
     "start_time": "2024-10-21T12:49:16.663278",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#import files\n",
    "import os\n",
    "import multiprocessing\n",
    "import pickle as pkl\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from collections import Counter, defaultdict\n",
    "import math\n",
    "import numpy as np\n",
    "from time import time\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7adc912",
   "metadata": {
    "papermill": {
     "duration": 0.006926,
     "end_time": "2024-10-21T12:49:20.193744",
     "exception": false,
     "start_time": "2024-10-21T12:49:20.186818",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 1. Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cae1fac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-21T12:49:20.210802Z",
     "iopub.status.busy": "2024-10-21T12:49:20.210040Z",
     "iopub.status.idle": "2024-10-21T12:49:20.271904Z",
     "shell.execute_reply": "2024-10-21T12:49:20.270472Z"
    },
    "papermill": {
     "duration": 0.074272,
     "end_time": "2024-10-21T12:49:20.275143",
     "exception": false,
     "start_time": "2024-10-21T12:49:20.200871",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "CORES = multiprocessing.cpu_count()\n",
    "\n",
    "# Path to the data folder\n",
    "DATA_FOLDER = \"/kaggle/input/\"\n",
    "\n",
    "#Path for pickles\n",
    "PICKLES_FOLDER = os.path.join(DATA_FOLDER, \"pickle-files-dis\")\n",
    "\n",
    "#Path for the stopwords\n",
    "STOPWORDS_FOLDER = os.path.join(DATA_FOLDER, \"stopwords\")\n",
    "\n",
    "def load_stopwords(path):\n",
    "    with open(path, 'r') as f:\n",
    "        arabic_stopwords = f.read().splitlines()\n",
    "    return arabic_stopwords\n",
    "\n",
    "STOP_WORDS = {\n",
    "    \"en\": set(load_stopwords(os.path.join(STOPWORDS_FOLDER, \"english\"))),\n",
    "    \"fr\": set(load_stopwords(os.path.join(STOPWORDS_FOLDER, \"french\"))),\n",
    "    \"de\": set(load_stopwords(os.path.join(STOPWORDS_FOLDER, \"german\"))),\n",
    "    \"es\": set(load_stopwords(os.path.join(STOPWORDS_FOLDER, \"spanish\"))),\n",
    "    \"it\": set(load_stopwords(os.path.join(STOPWORDS_FOLDER, \"italian\"))),\n",
    "    \"ko\": set(load_stopwords(os.path.join(STOPWORDS_FOLDER, \"korean\"))),\n",
    "    \"ar\": set(load_stopwords(os.path.join(STOPWORDS_FOLDER, \"arabic\"))),\n",
    "}\n",
    "\n",
    "#Path for the corpus\n",
    "CORPUS = os.path.join(DATA_FOLDER, \"dis-project-1-document-retrieval\", \"corpus.json\")\n",
    "QUERIES = os.path.join(DATA_FOLDER,\"dis-project-1-document-retrieval\", \"test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b33c71",
   "metadata": {
    "papermill": {
     "duration": 0.00714,
     "end_time": "2024-10-21T12:49:20.289671",
     "exception": false,
     "start_time": "2024-10-21T12:49:20.282531",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 2. Util functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8b9007c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-21T12:49:20.305868Z",
     "iopub.status.busy": "2024-10-21T12:49:20.305316Z",
     "iopub.status.idle": "2024-10-21T12:49:20.319356Z",
     "shell.execute_reply": "2024-10-21T12:49:20.317813Z"
    },
    "papermill": {
     "duration": 0.025645,
     "end_time": "2024-10-21T12:49:20.322324",
     "exception": false,
     "start_time": "2024-10-21T12:49:20.296679",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_data(data: any, file_name: str, folder: str = os.path.join(DATA_FOLDER, \"pickles\")):\n",
    "    \"\"\"Save the data to a file\n",
    "    \n",
    "    Args:\n",
    "        * data (any): the data to save\n",
    "\n",
    "        * file_name (str): the name of the file\n",
    "\n",
    "        * folder (str): the folder where to save the file\n",
    "    \"\"\"\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "\n",
    "    file_path = os.path.join(folder, file_name)\n",
    "\n",
    "    with open(file_path, 'wb') as handle:\n",
    "        pkl.dump(data, handle)\n",
    "\n",
    "def load_data(file_name: str, folder: str = os.path.join(DATA_FOLDER, \"pickles\")) -> any:\n",
    "    \"\"\"Load the data from a file\n",
    "\n",
    "    Args:\n",
    "        * file_name (str): the name of the file\n",
    "\n",
    "        * folder (str): the folder where to save the file\n",
    "\n",
    "    Returns:\n",
    "        * any: the data\n",
    "    \"\"\"\n",
    "    file_path = os.path.join(folder, file_name)\n",
    "\n",
    "    with open(file_path, 'rb') as handle:\n",
    "        data = pkl.load(handle)\n",
    "\n",
    "    return data\n",
    "\n",
    "def create_term_to_id(tokens_list):\n",
    "    term_to_id = {}\n",
    "    for document in tqdm(tokens_list):\n",
    "        for term in document:\n",
    "            if term not in term_to_id:\n",
    "                term_to_id[term] = len(term_to_id)\n",
    "    return term_to_id\n",
    "\n",
    "def transform_query_to_int(query, term_to_id):\n",
    "    query_int = []\n",
    "    for term in query:\n",
    "        if term in term_to_id:\n",
    "            query_int.append(term_to_id[term])\n",
    "    return query_int"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf50e8c9",
   "metadata": {
    "papermill": {
     "duration": 0.007106,
     "end_time": "2024-10-21T12:49:20.336506",
     "exception": false,
     "start_time": "2024-10-21T12:49:20.329400",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 3. Processing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ea2102b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-21T12:49:20.354792Z",
     "iopub.status.busy": "2024-10-21T12:49:20.354229Z",
     "iopub.status.idle": "2024-10-21T12:49:20.369588Z",
     "shell.execute_reply": "2024-10-21T12:49:20.368007Z"
    },
    "papermill": {
     "duration": 0.027838,
     "end_time": "2024-10-21T12:49:20.372985",
     "exception": false,
     "start_time": "2024-10-21T12:49:20.345147",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def tokenize(text:str, lang:str=\"en\") -> list:\n",
    "    \"\"\"Tokenizes and stems the input text efficiently.\n",
    "\n",
    "    Args:\n",
    "        * text(str): The text to tokenize.\n",
    "\n",
    "        * lang(str): The language of the text. Defaults to \"en\".\n",
    "\n",
    "    Returns:\n",
    "        * list: The list of stemmed tokens.\n",
    "    \"\"\"\n",
    "    \n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    \n",
    "    # Combine stemming and stopword filtering into one pass for efficiency\n",
    "    return [stemmer.stem(word.lower()) for word in tokens if word.lower() not in STOP_WORDS[lang] or stemmer.stem(word.lower()) not in STOP_WORDS[lang]]\n",
    "\n",
    "def tokenize_documents(file_name:str, corpus:pd.DataFrame, drop_text:bool=True, verbose:bool=False) -> pd.DataFrame:\n",
    "    \"\"\"Tokenize the corpus\n",
    "\n",
    "    Args:\n",
    "        * file_name(str): The name of the file to save the tokenized corpus.\n",
    "\n",
    "        * corpus(pd.DataFrame): The corpus to tokenize.\n",
    "    \n",
    "    Returns:\n",
    "        * pd.DataFrame: The tokenized corpus.\n",
    "    \"\"\"\n",
    "    tqdm.pandas() \n",
    "    if os.path.exists(os.path.join(PICKLES_FOLDER, file_name + \"_tokenized.pkl\")) and os.path.exists(os.path.join(PICKLES_FOLDER, file_name + \"_tokens_list.pkl\")):\n",
    "        if verbose:\n",
    "            print(\"Loading tokenized corpus from pickle\")\n",
    "        corpus = load_data(file_name + \"_tokenized.pkl\", PICKLES_FOLDER)\n",
    "        tokens_list = load_data(file_name + \"_tokens_list.pkl\", PICKLES_FOLDER)\n",
    "    else: \n",
    "        if verbose:\n",
    "            print(\"Tokenizing corpus\")\n",
    "        corpus[\"tokens\"] = corpus.progress_apply(lambda row: tokenize(row['text'], lang=row['lang']), axis=1)\n",
    "        if drop_text:\n",
    "            corpus.drop(columns=[\"text\"], inplace=True)\n",
    "        tokens_list = corpus[\"tokens\"].tolist()\n",
    "        #save_data(corpus, file_name + \"_tokenized.pkl\", PICKLES_FOLDER)\n",
    "        #save_data(tokens_list, file_name + \"_tokens_list.pkl\", PICKLES_FOLDER)\n",
    "    return corpus, tokens_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6167d6",
   "metadata": {
    "papermill": {
     "duration": 0.007114,
     "end_time": "2024-10-21T12:49:20.388197",
     "exception": false,
     "start_time": "2024-10-21T12:49:20.381083",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 4. Score function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2cf5ffa5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-21T12:49:20.406010Z",
     "iopub.status.busy": "2024-10-21T12:49:20.405319Z",
     "iopub.status.idle": "2024-10-21T12:49:20.416276Z",
     "shell.execute_reply": "2024-10-21T12:49:20.414902Z"
    },
    "papermill": {
     "duration": 0.023801,
     "end_time": "2024-10-21T12:49:20.419538",
     "exception": false,
     "start_time": "2024-10-21T12:49:20.395737",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def bm25_score(\n",
    "    query:list, \n",
    "    document_id:int, \n",
    "    idf:dict, \n",
    "    tf:dict,\n",
    "    length_norm:float, \n",
    "    k1:float=1.5\n",
    ") -> float:\n",
    "    \"\"\"Compute the BM25 score for a given query and the document position in the corpus\n",
    "\n",
    "    Args:\n",
    "        * query(list): The list of query terms.\n",
    "\n",
    "        * document_id(int): The document position in the corpus.\n",
    "\n",
    "        * idf(dict): The inverse document frequency of the terms.\n",
    "\n",
    "        * tf(dict): The term frequency of the terms in the documents.\n",
    "\n",
    "        * length_norm(float): The length normalization of the document.\n",
    "\n",
    "        * k1(float): The BM25 parameter k1. Defaults to 1.5.\n",
    "    \n",
    "    Returns:\n",
    "        * float: The BM25 score.\n",
    "    \"\"\"\n",
    "    if document_id not in tf:\n",
    "        return 0\n",
    "    \n",
    "    score = 0\n",
    "\n",
    "    query_terms = set(query)  # Use set to avoid redundant term checks\n",
    "    for term in query_terms:\n",
    "        if term in tf[document_id]:\n",
    "            idf_term = idf[term]\n",
    "            tf_term = tf[document_id][term]\n",
    "            score += idf_term * (tf_term * (k1 + 1) / (tf_term + length_norm))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d60608f",
   "metadata": {
    "papermill": {
     "duration": 0.007732,
     "end_time": "2024-10-21T12:49:20.434721",
     "exception": false,
     "start_time": "2024-10-21T12:49:20.426989",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 5. CorpusBase class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09eb7b79",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-21T12:49:20.453592Z",
     "iopub.status.busy": "2024-10-21T12:49:20.453064Z",
     "iopub.status.idle": "2024-10-21T12:49:20.477284Z",
     "shell.execute_reply": "2024-10-21T12:49:20.475331Z"
    },
    "papermill": {
     "duration": 0.037365,
     "end_time": "2024-10-21T12:49:20.480696",
     "exception": false,
     "start_time": "2024-10-21T12:49:20.443331",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CorpusBase:\n",
    "    def __init__(self, corpus_path: str, query_path: str, verbose: bool=False):\n",
    "        \"\"\" Initialize the CorpusBase object.\n",
    "        Args:\n",
    "            * corpus_path (str): the path to the corpus file.\n",
    "\n",
    "            * query_path (str): the path to the query file.\n",
    "\n",
    "            * verbose (bool): whether to print the progress or not. Defaults to False.\n",
    "\n",
    "        Class attributes:\n",
    "            * corpus (pd.DataFrame): the corpus.\n",
    "\n",
    "            * corpus_path (str): the path to the corpus file.\n",
    "\n",
    "            * corpus_file_name (str): the name of the corpus file.\n",
    "\n",
    "            * query_file_name (str): the name of the query file.\n",
    "\n",
    "            * tokens_list (list): the tokens list.\n",
    "\n",
    "            * results (list): the results of the queries.\n",
    "\n",
    "            * query (pd.DataFrame): the query.\n",
    "\n",
    "            * query_tokens_list (list): the query tokens list.\n",
    "\n",
    "            * query_path (str): the path to the query file.\n",
    "\n",
    "            * verbose (bool): whether to print the progress or not.\n",
    "        \"\"\"\n",
    "        self.corpus = None\n",
    "        self.corpus_path = corpus_path\n",
    "        self.corpus_file_name = corpus_path.split('/')[-1].split('.')[0]\n",
    "        self.query_file_name = query_path.split('/')[-1].split('.')[0]\n",
    "        self.tokens_list = None \n",
    "        self.results = None\n",
    "        self.query = None\n",
    "        self.query_tokens_list = None\n",
    "        self.query_path = query_path\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def load_corpus(self):\n",
    "        \"\"\"Load the corpus\n",
    "        \"\"\"\n",
    "        if os.path.exists(os.path.join(PICKLES_FOLDER, self.corpus_file_name + \".pkl\")):\n",
    "            if self.verbose:\n",
    "                print(\"Loading corpus from pickle\")\n",
    "            self.corpus = load_data(self.corpus_file_name + \".pkl\", PICKLES_FOLDER)\n",
    "        else:\n",
    "            if self.verbose:\n",
    "                print(f\"Loading corpus from {self.corpus_path}\")\n",
    "            if '.csv' in self.corpus_path:\n",
    "                self.corpus = pd.read_csv(self.corpus_path)\n",
    "            elif '.json' in self.corpus_path:\n",
    "                self.corpus = pd.read_json(self.corpus_path)\n",
    "            else:\n",
    "                raise ValueError(\"The file format is not supported\")\n",
    "            #save_data(self.corpus, self.corpus_file_name + \".pkl\", PICKLES_FOLDER)\n",
    "    \n",
    "    def load_query(self):\n",
    "        \"\"\"Load the query\n",
    "        \"\"\"\n",
    "        if os.path.exists(os.path.join(PICKLES_FOLDER, self.query_file_name + \".pkl\")):\n",
    "            if self.verbose:\n",
    "                print(\"Loading query from pickle\")\n",
    "            self.query = load_data(self.query_file_name + \".pkl\", PICKLES_FOLDER)\n",
    "        else:\n",
    "            if self.verbose:\n",
    "                print(f\"Loading query from {self.query_path}\")\n",
    "            if '.csv' in self.query_path:\n",
    "                self.query = pd.read_csv(self.query_path)\n",
    "\n",
    "    def tokenize_corpus(self, drop_text:bool=True):\n",
    "        \"\"\"Tokenize the corpus\n",
    "\n",
    "        Args:\n",
    "            * drop_text (bool): whether to drop the text column or not.\n",
    "        \"\"\"\n",
    "        if self.corpus is None:\n",
    "            self.load_corpus()\n",
    "        self.corpus, self.tokens_list = tokenize_documents(self.corpus_file_name, self.corpus, drop_text, self.verbose)\n",
    "    \n",
    "    def tokenize_query(self, drop_text:bool=True):\n",
    "        \"\"\"Tokenize the query\n",
    "\n",
    "        Args:\n",
    "            * drop_text (bool): whether to drop the text column or not.\n",
    "        \"\"\"\n",
    "        if self.query is None:\n",
    "            self.load_query()\n",
    "        if 'query' in self.query.columns:\n",
    "            self.query.rename(columns={'query': 'text'}, inplace=True)\n",
    "        self.query, self.query_tokens_list = tokenize_documents(self.query_file_name, self.query, drop_text, self.verbose) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad58b0a9",
   "metadata": {
    "papermill": {
     "duration": 0.006581,
     "end_time": "2024-10-21T12:49:20.494445",
     "exception": false,
     "start_time": "2024-10-21T12:49:20.487864",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 6. CorpusBm25 class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b37f5d27",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-21T12:49:20.511567Z",
     "iopub.status.busy": "2024-10-21T12:49:20.511079Z",
     "iopub.status.idle": "2024-10-21T12:49:20.576992Z",
     "shell.execute_reply": "2024-10-21T12:49:20.575608Z"
    },
    "papermill": {
     "duration": 0.079081,
     "end_time": "2024-10-21T12:49:20.580427",
     "exception": false,
     "start_time": "2024-10-21T12:49:20.501346",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CorpusBm25(CorpusBase):\n",
    "    def __init__(\n",
    "            self, corpus_path: str, \n",
    "            query_path: str, \n",
    "            k1:float=1.5, \n",
    "            b:float=0.75, \n",
    "            filter:bool=False,\n",
    "            filt_docs:int=10000,\n",
    "            verbose:bool=True):\n",
    "        \"\"\"\n",
    "        Initialize the CorpusBM25 object.\n",
    "\n",
    "        Args:\n",
    "            * corpus_path: str, the path to the corpus file.\n",
    "\n",
    "            * query_path: str, the path to the query file.\n",
    "\n",
    "            * k1: float, the BM25 parameter k1. Defaults to 1.5.\n",
    "\n",
    "            * b: float, the BM25 parameter b. Defaults to 0.75.\n",
    "\n",
    "            * filter: bool, whether to filter the results or not. Defaults to False.\n",
    "\n",
    "            * filt_docs: int, the number of documents to filter. Defaults to 10000.\n",
    "\n",
    "            * verbose: bool, whether to print the progress or not. Defaults to False.\n",
    "\n",
    "        Class attributes:\n",
    "            * tf (dict): the term frequency for each term in each document.\n",
    "\n",
    "            * idf (dict): the inverse document frequency for each term.\n",
    "\n",
    "            * df (dict): the document frequency for each term.\n",
    "\n",
    "            * avg_doc_len (float): the average document length.\n",
    "\n",
    "            * doc_len (list): the length of each document in the corpus.\n",
    "\n",
    "            * results (list): the results of the queries.\n",
    "\n",
    "            * k1 (float): the BM25 parameter k1.\n",
    "\n",
    "            * b (float): the BM25 parameter b.\n",
    "\n",
    "            * filter (bool): whether to filter the results or not.\n",
    "\n",
    "            * filt_docs (int): the number of documents to filter.\n",
    "\n",
    "            * inverted_index (dict): the inverted index of the corpus.\n",
    "\n",
    "            * term_to_id (dict): the mapping of terms to IDs.\n",
    "\n",
    "            * time (float): the time taken to process the queries and calculate the BM25 scores.\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__(corpus_path, query_path)\n",
    "        self.tf = None\n",
    "        self.idf = None\n",
    "        self.df = None\n",
    "        self.avg_doc_len = None\n",
    "        self.doc_len = None\n",
    "        self.results = None\n",
    "        self.inverted_index = None\n",
    "        self.term_to_id = None\n",
    "        self.k1 = k1\n",
    "        self.b = b\n",
    "        self.filter = filter\n",
    "        self.filt_docs = int(filt_docs)\n",
    "        self.time = None\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def _compute_df(self):\n",
    "        \"\"\"Compute the document frequency for each term in the corpus (i.e., the number of documents in which the term appears).\n",
    "        \"\"\"\n",
    "        if self.df is None:\n",
    "            if os.path.exists(os.path.join(PICKLES_FOLDER, self.corpus_file_name + \"_df.pkl\")) \\\n",
    "                and os.path.exists(os.path.join(PICKLES_FOLDER, self.corpus_file_name + \"_term_to_id.pkl\")):\n",
    "                if self.verbose:\n",
    "                    print(\"Loading df from pickle\")\n",
    "                self.df = load_data(self.corpus_file_name + \"_df.pkl\", PICKLES_FOLDER)\n",
    "                self.term_to_id = load_data(self.corpus_file_name + \"_term_to_id.pkl\", PICKLES_FOLDER)\n",
    "            \n",
    "            else:\n",
    "                if self.corpus is None:\n",
    "                    self.load_corpus()\n",
    "                if 'tokens' not in self.corpus.columns:\n",
    "                    self.tokenize_corpus()\n",
    "                if self.term_to_id is None:\n",
    "                    self.term_to_id = create_term_to_id(self.corpus['tokens'].tolist())\n",
    "                    #save_data(self.term_to_id, self.corpus_file_name + \"_term_to_id.pkl\", PICKLES_FOLDER)\n",
    "                self.corpus['tokens'] = self.corpus['tokens'].apply(lambda x: transform_query_to_int(x, self.term_to_id))\n",
    "                if self.verbose:\n",
    "                    print(\"Computing df\")\n",
    "                corpus_tokenized = self.corpus['tokens'].tolist()\n",
    "                self.df = Counter(term for document in corpus_tokenized for term in set(document))\n",
    "                #save_data(self.df, self.corpus_file_name + \"_df.pkl\", PICKLES_FOLDER)   \n",
    "    \n",
    "    def _compute_idf(self):\n",
    "        \"\"\"Compute the inverse document frequency for each term in the corpus.\n",
    "        \"\"\"\n",
    "        if self.idf is None:\n",
    "            if os.path.exists(os.path.join(PICKLES_FOLDER, self.corpus_file_name + \"_idf.pkl\")):\n",
    "                if self.verbose:\n",
    "                    print(\"Loading idf from pickle\")\n",
    "                self.idf = load_data(self.corpus_file_name + \"_idf.pkl\", PICKLES_FOLDER)    \n",
    "            else:\n",
    "                if self.df is None:\n",
    "                    self._compute_df()\n",
    "                if self.corpus is None:\n",
    "                    self.load_corpus()\n",
    "                if self.verbose:\n",
    "                    print(\"Computing idf\")\n",
    "                num_documents = len(self.corpus)\n",
    "                self.idf = {term: math.log(1 + (num_documents - self.df[term] + 0.5) / (self.df[term] + 0.5)) for term in self.df}\n",
    "                #save_data(self.idf, self.corpus_file_name + \"_idf.pkl\", PICKLES_FOLDER)\n",
    "    \n",
    "    def _compute_tf(self):\n",
    "        \"\"\"Compute the term frequency for each term in each document.\n",
    "        \"\"\"\n",
    "        if self.tf is None:\n",
    "            if os.path.exists(os.path.join(PICKLES_FOLDER, self.corpus_file_name + \"_tf.pkl\")):\n",
    "                if self.verbose:\n",
    "                    print(\"Loading tf from pickle\")\n",
    "                self.tf = load_data(self.corpus_file_name + \"_tf.pkl\", PICKLES_FOLDER)\n",
    "            else:\n",
    "                if self.corpus is None:\n",
    "                    self.load_corpus()\n",
    "                if 'tokens' not in self.corpus.columns:\n",
    "                    self.tokenize_corpus()\n",
    "                if self.verbose:\n",
    "                    print(\"Computing tf\")\n",
    "                corpus_tokenized = self.corpus['tokens'].tolist()\n",
    "                self.tf = {i: dict(Counter(document)) for i, document in enumerate(corpus_tokenized)}\n",
    "                #save_data(self.tf, self.corpus_file_name + \"_tf.pkl\", PICKLES_FOLDER)\n",
    "    \n",
    "    def _compute_doc_len(self):\n",
    "        \"\"\"Compute the length of each document in the corpus.\n",
    "        \"\"\"\n",
    "        if self.doc_len is None or self.avg_doc_len is None:\n",
    "            if os.path.exists(os.path.join(PICKLES_FOLDER, self.corpus_file_name + \"_doc_len.pkl\")) \\\n",
    "                and os.path.exists(os.path.join(PICKLES_FOLDER, self.corpus_file_name + \"_avg_doc_len.pkl\")):\n",
    "                if self.verbose:\n",
    "                    print(\"Loading doc_len from pickle\")\n",
    "                self.doc_len = load_data(self.corpus_file_name + \"_doc_len.pkl\", PICKLES_FOLDER)\n",
    "                self.avg_doc_len = load_data(self.corpus_file_name + \"_avg_doc_len.pkl\", PICKLES_FOLDER)\n",
    "            else:\n",
    "                if self.corpus is None:\n",
    "                    self.load_corpus()\n",
    "                if 'tokens' not in self.corpus.columns:\n",
    "                    self.tokenize_corpus()\n",
    "                if self.verbose:\n",
    "                    print(\"Computing doc_len\")\n",
    "                self.doc_len = [len(document) for document in self.corpus['tokens'].tolist()]\n",
    "                self.avg_doc_len = sum(self.doc_len) / len(self.doc_len)\n",
    "                #save_data(self.doc_len, self.corpus_file_name + \"_doc_len.pkl\", PICKLES_FOLDER)\n",
    "                #save_data(self.avg_doc_len, self.corpus_file_name + \"_avg_doc_len.pkl\", PICKLES_FOLDER)\n",
    "\n",
    "    def _compute_length_norm(self):\n",
    "        \"\"\"Compute the length normalization factor for a given document length.\n",
    "\n",
    "        Args:\n",
    "            * doc_len (int): the length of the document.\n",
    "\n",
    "            * avg_doc_len (float): the average document length.\n",
    "\n",
    "            * k1 (float): the BM25 parameter k1. Defaults to 1.5.\n",
    "\n",
    "            * b (float): the BM25 parameter b. Defaults to 0.75.\n",
    "        \"\"\"\n",
    "        if self.doc_len is None:\n",
    "            self._compute_doc_len()\n",
    "        if self.avg_doc_len is None:\n",
    "            self._compute_doc_len()\n",
    "        if self.verbose:\n",
    "            print(\"Computing length_norm\")\n",
    "        self.length_norm = [self.k1 * (1 - self.b + self.b * doc_length / self.avg_doc_len) for doc_length in self.doc_len]\n",
    "    \n",
    "    def _compute_inverted_index(self):\n",
    "        if os.path.exists(os.path.join(PICKLES_FOLDER, self.corpus_file_name + \"_inverted_index.pkl\")):\n",
    "            if self.verbose:\n",
    "                print(\"Loading inverted index from pickle\")\n",
    "            self.inverted_index = load_data(self.corpus_file_name + \"_inverted_index.pkl\", PICKLES_FOLDER)\n",
    "        else:\n",
    "            if self.tf is None:\n",
    "                self._compute_tf()\n",
    "            self.inverted_index = {}\n",
    "            if self.verbose:\n",
    "                for doc_id, doc in tqdm(self.tf.items(), total=len(self.tf), desc=\"Computing inverted index\"):\n",
    "                    for word, _ in doc.items():\n",
    "                        if word not in self.inverted_index:\n",
    "                            self.inverted_index[word] = []\n",
    "                        self.inverted_index[word].append(doc_id)\n",
    "            else:\n",
    "                for doc_id, doc in self.tf.items():\n",
    "                    for word, _ in doc.items():\n",
    "                        if word not in self.inverted_index:\n",
    "                            self.inverted_index[word] = []\n",
    "                        self.inverted_index[word].append(doc_id)\n",
    "            #save_data(self.inverted_index, self.corpus_file_name + \"_inverted_index.pkl\", PICKLES_FOLDER)\n",
    "    \n",
    "    def _BM25_search(self,query, docid,relevant_docs, k=10):\n",
    "        \"\"\"Compute BM25 score for all documents in the corpus for a given query and language and return the top-k documents\n",
    "\n",
    "        Args:\n",
    "            * query (list): the tokenized query.\n",
    "\n",
    "            * docid (list): the list of document IDs.\n",
    "\n",
    "            * relevant_docs (list): the list of relevant document IDs.\n",
    "\n",
    "            * k (int): the number of documents to return.\n",
    "\n",
    "        Returns:\n",
    "            * top_doc_ids (list): the list of document IDs with the highest BM25 scores.\n",
    "        \"\"\"\n",
    "\n",
    "        # Calculate scores only for relevant documents\n",
    "        scores = []\n",
    "        for i in relevant_docs:\n",
    "            length_norm = self.length_norm[i]\n",
    "            score = bm25_score(query, i, self.idf, self.tf, length_norm, self.k1)\n",
    "            scores.append((score, docid[i]))\n",
    "\n",
    "        # Sort and get top-k documents by score\n",
    "        scores.sort(key=lambda x: -x[0])\n",
    "        top_doc_ids = [doc_id for _, doc_id in scores[:k]]\n",
    "        \n",
    "        return top_doc_ids\n",
    "    \n",
    "    def _get_relevant_docs(self, query, relevant_docs, k=10000):\n",
    "        query_test = set(query)\n",
    "        rel_docs = defaultdict(int)\n",
    "        for word in query_test:\n",
    "            if word in self.inverted_index:\n",
    "                for doc_id in self.inverted_index[word]:\n",
    "                    rel_docs[doc_id] += 1\n",
    "        sorted_rel_docs = np.array(sorted(rel_docs.items(), key=lambda x: x[1], reverse=True))\n",
    "        rel_docs_ids = np.intersect1d(sorted_rel_docs, relevant_docs)\n",
    "        rel_docs_ids = rel_docs_ids[:k] if len(rel_docs_ids) > k else rel_docs_ids\n",
    "        return rel_docs_ids\n",
    "\n",
    "    \n",
    "    def get_results(self):\n",
    "        \"\"\"Get the results of the queries\n",
    "        \"\"\"\n",
    "        self.results = []\n",
    "\n",
    "        #initialize the idf, tf, avg_doc_len, doc_len\n",
    "        if self.verbose:\n",
    "            print(\"Computing idf, tf, avg_doc_len, doc_len\")\n",
    "        self._compute_df()\n",
    "        self._compute_idf()\n",
    "        self._compute_tf()\n",
    "        self._compute_doc_len()\n",
    "        self._compute_length_norm()\n",
    "        if self.filter:\n",
    "            self._compute_inverted_index()\n",
    "        \n",
    "        if os.path.exists(os.path.join(PICKLES_FOLDER, self.corpus_file_name + \"_docid.pkl\")):\n",
    "            if self.verbose:\n",
    "                print(\"Loading docid from pickle\")\n",
    "            docid = load_data(self.corpus_file_name + \"_docid.pkl\", PICKLES_FOLDER)\n",
    "        if os.path.exists(os.path.join(PICKLES_FOLDER, self.corpus_file_name + \"_lang.pkl\")):\n",
    "            if self.verbose:\n",
    "                print(\"Loading lang from pickle\")\n",
    "            lang = load_data(self.corpus_file_name + \"_lang.pkl\", PICKLES_FOLDER)\n",
    "        else:\n",
    "            #load the corpus\n",
    "            if self.corpus is None:\n",
    "                self.load_corpus()\n",
    "\n",
    "            #extract list of docid, lang and tokenized text from the corpus\n",
    "            docid = self.corpus['docid'].tolist()\n",
    "            #save_data(docid, self.corpus_file_name + \"_docid.pkl\", PICKLES_FOLDER)\n",
    "            lang = self.corpus['lang'].tolist()\n",
    "            #save_data(lang, self.corpus_file_name + \"_lang.pkl\", PICKLES_FOLDER)\n",
    "\n",
    "        # Create a dictionary with the relevant documents for each language\n",
    "        langs = set(lang)\n",
    "        dict_relevant_docs = {l: [i for i in range(len(docid)) if lang[i] == l] for l in langs}\n",
    "\n",
    "        start = time()\n",
    "\n",
    "        # Load the queries\n",
    "        if self.query is None:\n",
    "            self.load_query()\n",
    "        if 'tokens' not in self.query.columns:\n",
    "            self.tokenize_query()\n",
    "            self.query['tokens'] = self.query['tokens'].apply(lambda x: transform_query_to_int(x, self.term_to_id))\n",
    "\n",
    "        #extract list of tokenized text and lang from the test queries\n",
    "        list_test_queries = self.query[\"tokens\"].tolist()\n",
    "        list_lang_test_queries = self.query[\"lang\"].tolist()\n",
    "\n",
    "        # Loop over each query and calculate the BM25 scores\n",
    "        if self.verbose:\n",
    "            for idx, query in tqdm(enumerate(list_test_queries), total=len(list_test_queries), desc=\"Calculating BM25 scores\"):\n",
    "                query_lang = list_lang_test_queries[idx]  # Get the language for the current query\n",
    "                \n",
    "                # Get the top 10 documents for the current query\n",
    "                relevant_docs = np.array(dict_relevant_docs[query_lang])\n",
    "                if self.filter:\n",
    "                    relevant_docs = self._get_relevant_docs(query, relevant_docs, self.filt_docs)\n",
    "                top_docs = self._BM25_search(query, docid,relevant_docs, k=10)\n",
    "                \n",
    "                # Append the result as a dictionary\n",
    "                self.results.append({\n",
    "                    'id': idx,  # You may replace idx with actual query ID if available\n",
    "                    'docids': top_docs\n",
    "                })  \n",
    "        else:\n",
    "            for idx, query in enumerate(list_test_queries):\n",
    "                query_lang = list_lang_test_queries[idx]  # Get the language for the current query\n",
    "                \n",
    "                # Get the top 10 documents for the current query\n",
    "                relevant_docs = np.array(dict_relevant_docs[query_lang])\n",
    "                if self.filter:\n",
    "                    relevant_docs = self._get_relevant_docs(query, relevant_docs, self.filt_docs)\n",
    "                top_docs = self._BM25_search(query, docid,relevant_docs, k=10)\n",
    "                \n",
    "                # Append the result as a dictionary\n",
    "                self.results.append({\n",
    "                    'id': idx,  # You may replace idx with actual query ID if available\n",
    "                    'docids': top_docs\n",
    "                }) \n",
    "        end = time()\n",
    "        if self.verbose:\n",
    "            print(f\"Time taken to process queries and compute BM25 scores: {int((end - start) / 60)} min {int((end - start) % 60)} sec\")\n",
    "        self.time = end - start\n",
    "\n",
    "    def create_submission(self, output_path: str):\n",
    "        \"\"\" Create a submission file for the BM25 model.\n",
    "\n",
    "        Args:\n",
    "            * output_path (str): the path to the output file.\n",
    "        \"\"\"\n",
    "        if self.results is None:\n",
    "            self.get_results()\n",
    "        results_df = pd.DataFrame(self.results)\n",
    "        results_df.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1f0d50",
   "metadata": {
    "papermill": {
     "duration": 0.007737,
     "end_time": "2024-10-21T12:49:20.595456",
     "exception": false,
     "start_time": "2024-10-21T12:49:20.587719",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 7. Final Part: Computing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f6b5463",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-21T12:49:20.612381Z",
     "iopub.status.busy": "2024-10-21T12:49:20.611905Z",
     "iopub.status.idle": "2024-10-21T12:49:20.618288Z",
     "shell.execute_reply": "2024-10-21T12:49:20.616689Z"
    },
    "papermill": {
     "duration": 0.018659,
     "end_time": "2024-10-21T12:49:20.621589",
     "exception": false,
     "start_time": "2024-10-21T12:49:20.602930",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "documents = CorpusBm25(CORPUS, QUERIES, filter=True, filt_docs=10000, k1=2, b=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2cd34289",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-21T12:49:20.637744Z",
     "iopub.status.busy": "2024-10-21T12:49:20.637251Z",
     "iopub.status.idle": "2024-10-21T13:01:17.209467Z",
     "shell.execute_reply": "2024-10-21T13:01:17.207714Z"
    },
    "papermill": {
     "duration": 716.585048,
     "end_time": "2024-10-21T13:01:17.213719",
     "exception": false,
     "start_time": "2024-10-21T12:49:20.628671",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing idf, tf, avg_doc_len, doc_len\n",
      "Loading df from pickle\n",
      "Loading idf from pickle\n",
      "Loading tf from pickle\n",
      "Loading doc_len from pickle\n",
      "Computing length_norm\n",
      "Loading inverted index from pickle\n",
      "Loading docid from pickle\n",
      "Loading lang from pickle\n",
      "Loading query from /kaggle/input/dis-project-1-document-retrieval/test.csv\n",
      "Tokenizing corpus\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:01<00:00, 1661.68it/s]\n",
      "Calculating BM25 scores: 100%|██████████| 2000/2000 [09:23<00:00,  3.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to process queries and compute BM25 scores: 9 min 24 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "documents.create_submission(output_path='submission_bm25_v.csv')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 9635715,
     "sourceId": 85316,
     "sourceType": "competition"
    },
    {
     "datasetId": 5892229,
     "sourceId": 9647682,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5892189,
     "sourceId": 9653503,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 201329707,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 728.834014,
   "end_time": "2024-10-21T13:01:22.219141",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-10-21T12:49:13.385127",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
