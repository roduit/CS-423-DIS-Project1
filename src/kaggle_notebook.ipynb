{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e22c876",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T07:43:41.369918Z",
     "iopub.status.busy": "2024-10-17T07:43:41.368944Z",
     "iopub.status.idle": "2024-10-17T07:43:41.374685Z",
     "shell.execute_reply": "2024-10-17T07:43:41.373649Z"
    },
    "papermill": {
     "duration": 0.017623,
     "end_time": "2024-10-17T07:43:41.376992",
     "exception": false,
     "start_time": "2024-10-17T07:43:41.359369",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# -*- authors : Vincent Roduit -*-\n",
    "# -*- date : 2024-09-30 -*-\n",
    "# -*- Last revision: 2024-09-30 by Vincent Roduit -*-\n",
    "# -*- python version : 3.9.19 -*-\n",
    "# -*- Description: Constants used in the code *-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d19b774",
   "metadata": {
    "papermill": {
     "duration": 0.006466,
     "end_time": "2024-10-17T07:43:41.390161",
     "exception": false,
     "start_time": "2024-10-17T07:43:41.383695",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <center> CS - 423: Distributed Information Systems </center>\n",
    "## <center> Ecole Polytechnique Fédérale de Lausanne </center>\n",
    "### <center>Project 1: Document Retrieval </center>\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1834dc4c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T07:43:41.404645Z",
     "iopub.status.busy": "2024-10-17T07:43:41.404245Z",
     "iopub.status.idle": "2024-10-17T07:43:44.426841Z",
     "shell.execute_reply": "2024-10-17T07:43:44.425623Z"
    },
    "papermill": {
     "duration": 3.032911,
     "end_time": "2024-10-17T07:43:44.429493",
     "exception": false,
     "start_time": "2024-10-17T07:43:41.396582",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#import files\n",
    "import os\n",
    "import multiprocessing\n",
    "import pickle as pkl\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import math\n",
    "from time import time\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e80b011",
   "metadata": {
    "papermill": {
     "duration": 0.006009,
     "end_time": "2024-10-17T07:43:44.442050",
     "exception": false,
     "start_time": "2024-10-17T07:43:44.436041",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 1. Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d05c46f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T07:43:44.456835Z",
     "iopub.status.busy": "2024-10-17T07:43:44.456239Z",
     "iopub.status.idle": "2024-10-17T07:43:44.491140Z",
     "shell.execute_reply": "2024-10-17T07:43:44.490121Z"
    },
    "papermill": {
     "duration": 0.045289,
     "end_time": "2024-10-17T07:43:44.493730",
     "exception": false,
     "start_time": "2024-10-17T07:43:44.448441",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "CORES = multiprocessing.cpu_count()\n",
    "\n",
    "# Path to the data folder\n",
    "DATA_FOLDER = \"/kaggle/input/\"\n",
    "\n",
    "#Path for pickles\n",
    "PICKLES_FOLDER = os.path.join(DATA_FOLDER, \"pickle-files-dis\")\n",
    "\n",
    "#Path for the stopwords\n",
    "STOPWORDS_FOLDER = os.path.join(DATA_FOLDER, \"stopwords\")\n",
    "\n",
    "def load_stopwords(path):\n",
    "    with open(path, 'r') as f:\n",
    "        arabic_stopwords = f.read().splitlines()\n",
    "    return arabic_stopwords\n",
    "\n",
    "STOP_WORDS = {\n",
    "    \"en\": set(load_stopwords(os.path.join(STOPWORDS_FOLDER, \"english\"))),\n",
    "    \"fr\": set(load_stopwords(os.path.join(STOPWORDS_FOLDER, \"french\"))),\n",
    "    \"de\": set(load_stopwords(os.path.join(STOPWORDS_FOLDER, \"german\"))),\n",
    "    \"es\": set(load_stopwords(os.path.join(STOPWORDS_FOLDER, \"spanish\"))),\n",
    "    \"it\": set(load_stopwords(os.path.join(STOPWORDS_FOLDER, \"italian\"))),\n",
    "    \"ko\": set(load_stopwords(os.path.join(STOPWORDS_FOLDER, \"korean\"))),\n",
    "    \"ar\": set(load_stopwords(os.path.join(STOPWORDS_FOLDER, \"arabic\"))),\n",
    "}\n",
    "\n",
    "#Path for the corpus\n",
    "CORPUS = os.path.join(DATA_FOLDER, \"dis-project-1-document-retrieval\", \"corpus.json\")\n",
    "QUERIES = os.path.join(DATA_FOLDER,\"dis-project-1-document-retrieval\", \"test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8224fc30",
   "metadata": {
    "papermill": {
     "duration": 0.006004,
     "end_time": "2024-10-17T07:43:44.506214",
     "exception": false,
     "start_time": "2024-10-17T07:43:44.500210",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 2. Util functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c46169ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T07:43:44.521464Z",
     "iopub.status.busy": "2024-10-17T07:43:44.521050Z",
     "iopub.status.idle": "2024-10-17T07:43:44.530037Z",
     "shell.execute_reply": "2024-10-17T07:43:44.529029Z"
    },
    "papermill": {
     "duration": 0.018926,
     "end_time": "2024-10-17T07:43:44.532486",
     "exception": false,
     "start_time": "2024-10-17T07:43:44.513560",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_data(data: any, file_name: str, folder: str = os.path.join(DATA_FOLDER, \"pickles\")):\n",
    "    \"\"\"Save the data to a file\n",
    "    \n",
    "    Args:\n",
    "        * data (any): the data to save\n",
    "\n",
    "        * file_name (str): the name of the file\n",
    "\n",
    "        * folder (str): the folder where to save the file\n",
    "    \"\"\"\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "\n",
    "    file_path = os.path.join(folder, file_name)\n",
    "\n",
    "    with open(file_path, 'wb') as handle:\n",
    "        pkl.dump(data, handle)\n",
    "\n",
    "def load_data(file_name: str, folder: str = os.path.join(DATA_FOLDER, \"pickles\")) -> any:\n",
    "    \"\"\"Load the data from a file\n",
    "\n",
    "    Args:\n",
    "        * file_name (str): the name of the file\n",
    "\n",
    "        * folder (str): the folder where to save the file\n",
    "\n",
    "    Returns:\n",
    "        * any: the data\n",
    "    \"\"\"\n",
    "    file_path = os.path.join(folder, file_name)\n",
    "\n",
    "    with open(file_path, 'rb') as handle:\n",
    "        data = pkl.load(handle)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05d69b3",
   "metadata": {
    "papermill": {
     "duration": 0.005738,
     "end_time": "2024-10-17T07:43:44.544568",
     "exception": false,
     "start_time": "2024-10-17T07:43:44.538830",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 3. Processing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f829e70",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T07:43:44.558544Z",
     "iopub.status.busy": "2024-10-17T07:43:44.558136Z",
     "iopub.status.idle": "2024-10-17T07:43:44.569720Z",
     "shell.execute_reply": "2024-10-17T07:43:44.568807Z"
    },
    "papermill": {
     "duration": 0.021217,
     "end_time": "2024-10-17T07:43:44.571999",
     "exception": false,
     "start_time": "2024-10-17T07:43:44.550782",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def tokenize(text:str, lang:str=\"en\") -> list:\n",
    "    \"\"\"Tokenizes and stems the input text efficiently.\n",
    "\n",
    "    Args:\n",
    "        * text(str): The text to tokenize.\n",
    "\n",
    "        * lang(str): The language of the text. Defaults to \"en\".\n",
    "\n",
    "    Returns:\n",
    "        * list: The list of stemmed tokens.\n",
    "    \"\"\"\n",
    "    \n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    \n",
    "    # Combine stemming and stopword filtering into one pass for efficiency\n",
    "    return [stemmer.stem(word.lower()) for word in tokens if word.lower() not in STOP_WORDS[lang] or stemmer.stem(word.lower()) not in STOP_WORDS[lang]]\n",
    "\n",
    "def tokenize_documents(file_name:str, corpus:pd.DataFrame, drop_text:bool=True) -> pd.DataFrame:\n",
    "    \"\"\"Tokenize the corpus\n",
    "\n",
    "    Args:\n",
    "        * file_name(str): The name of the file to save the tokenized corpus.\n",
    "\n",
    "        * corpus(pd.DataFrame): The corpus to tokenize.\n",
    "    \n",
    "    Returns:\n",
    "        * pd.DataFrame: The tokenized corpus.\n",
    "    \"\"\"\n",
    "    tqdm.pandas() \n",
    "    if os.path.exists(os.path.join(PICKLES_FOLDER, file_name + \"_tokenized.pkl\")) and os.path.exists(os.path.join(PICKLES_FOLDER, file_name + \"_tokens_list.pkl\")):\n",
    "        print(\"Loading tokenized corpus from pickle\")\n",
    "        corpus = load_data(file_name + \"_tokenized.pkl\", PICKLES_FOLDER)\n",
    "        tokens_list = load_data(file_name + \"_tokens_list.pkl\", PICKLES_FOLDER)\n",
    "    else: \n",
    "        print(\"Tokenizing corpus\")\n",
    "        corpus[\"tokens\"] = corpus.progress_apply(lambda row: tokenize(row['text'], lang=row['lang']), axis=1)\n",
    "        if drop_text:\n",
    "            corpus.drop(columns=[\"text\"], inplace=True)\n",
    "        tokens_list = corpus[\"tokens\"].tolist()\n",
    "        #save_data(corpus, file_name + \"_tokenized.pkl\", PICKLES_FOLDER)\n",
    "        #save_data(tokens_list, file_name + \"_tokens_list.pkl\", PICKLES_FOLDER)\n",
    "    return corpus, tokens_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e00cfb0",
   "metadata": {
    "papermill": {
     "duration": 0.006169,
     "end_time": "2024-10-17T07:43:44.584547",
     "exception": false,
     "start_time": "2024-10-17T07:43:44.578378",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 4. Score function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd057a68",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T07:43:44.599032Z",
     "iopub.status.busy": "2024-10-17T07:43:44.598600Z",
     "iopub.status.idle": "2024-10-17T07:43:44.606877Z",
     "shell.execute_reply": "2024-10-17T07:43:44.605809Z"
    },
    "papermill": {
     "duration": 0.018135,
     "end_time": "2024-10-17T07:43:44.609158",
     "exception": false,
     "start_time": "2024-10-17T07:43:44.591023",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def bm25_score(query, document_id, idf, tf, avg_doc_len, doc_len, k1=1.5, b=0.75):\n",
    "    \"\"\"Compute the BM25 score for a given query and the document position in the corpus\n",
    "\n",
    "    Args:\n",
    "        * query(list): The list of query terms.\n",
    "\n",
    "        * document_id(int): The document position in the corpus.\n",
    "\n",
    "        * idf(dict): The inverse document frequency of the terms.\n",
    "\n",
    "        * tf(dict): The term frequency of the terms in the documents.\n",
    "\n",
    "        * avg_doc_len(float): The average document length.\n",
    "\n",
    "        * doc_len(dict): The length of the documents.\n",
    "\n",
    "        * k1(float): The BM25 parameter k1. Defaults to 1.5.\n",
    "\n",
    "        * b(float): The BM25 parameter b. Defaults to 0.75.\n",
    "    \n",
    "    Returns:\n",
    "        * float: The BM25 score.\n",
    "    \"\"\"\n",
    "    score = 0\n",
    "    doc_length = doc_len[document_id]\n",
    "    length_norm = k1 * (1 - b + b * doc_length / avg_doc_len)  # Precompute normalization factor\n",
    "\n",
    "    query_terms = set(query)  # Use set to avoid redundant term checks\n",
    "    for term in query_terms:\n",
    "        if document_id in tf and term in tf[document_id]:\n",
    "            idf_term = idf.get(term, 0)  # Use .get() to handle missing terms\n",
    "            tf_term = tf[document_id][term]\n",
    "            score += idf_term * (tf_term * (k1 + 1) / (tf_term + length_norm))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2e21d7",
   "metadata": {
    "papermill": {
     "duration": 0.005968,
     "end_time": "2024-10-17T07:43:44.621570",
     "exception": false,
     "start_time": "2024-10-17T07:43:44.615602",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 5. CorpusBase class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c52ccaa4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T07:43:44.636327Z",
     "iopub.status.busy": "2024-10-17T07:43:44.635904Z",
     "iopub.status.idle": "2024-10-17T07:43:44.653762Z",
     "shell.execute_reply": "2024-10-17T07:43:44.652838Z"
    },
    "papermill": {
     "duration": 0.028082,
     "end_time": "2024-10-17T07:43:44.656078",
     "exception": false,
     "start_time": "2024-10-17T07:43:44.627996",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CorpusBase:\n",
    "    def __init__(self, corpus_path: str, query_path: str):\n",
    "        \"\"\" Initialize the CorpusBase object.\n",
    "        Args:\n",
    "            * corpus_path (str): the path to the corpus file.\n",
    "\n",
    "            * query_path (str): the path to the query file.\n",
    "\n",
    "        Class attributes:\n",
    "            * corpus (pd.DataFrame): the corpus.\n",
    "\n",
    "            * corpus_path (str): the path to the corpus file.\n",
    "\n",
    "            * corpus_file_name (str): the name of the corpus file.\n",
    "\n",
    "            * query_file_name (str): the name of the query file.\n",
    "\n",
    "            * tokens_list (list): the tokens list.\n",
    "\n",
    "            * results (list): the results of the queries.\n",
    "\n",
    "            * query (pd.DataFrame): the query.\n",
    "\n",
    "            * query_tokens_list (list): the query tokens list.\n",
    "\n",
    "            * query_path (str): the path to the query file.\n",
    "        \"\"\"\n",
    "        self.corpus = None\n",
    "        self.corpus_path = corpus_path\n",
    "        self.corpus_file_name = corpus_path.split('/')[-1].split('.')[0]\n",
    "        self.query_file_name = query_path.split('/')[-1].split('.')[0]\n",
    "        self.tokens_list = None \n",
    "        self.results = None\n",
    "        self.query = None\n",
    "        self.query_tokens_list = None\n",
    "        self.query_path = query_path\n",
    "\n",
    "    def load_corpus(self):\n",
    "        \"\"\"Load the corpus\n",
    "        \"\"\"\n",
    "        if os.path.exists(os.path.join(PICKLES_FOLDER, self.corpus_file_name + \".pkl\")):\n",
    "            print(\"Loading corpus from pickle\")\n",
    "            self.corpus = load_data(self.corpus_file_name + \".pkl\", PICKLES_FOLDER)\n",
    "        else:\n",
    "            print(f\"Loading corpus from {self.corpus_path}\")\n",
    "            if '.csv' in self.corpus_path:\n",
    "                self.corpus = pd.read_csv(self.corpus_path)\n",
    "            elif '.json' in self.corpus_path:\n",
    "                self.corpus = pd.read_json(self.corpus_path)\n",
    "            else:\n",
    "                raise ValueError(\"The file format is not supported\")\n",
    "    \n",
    "    def load_query(self):\n",
    "        \"\"\"Load the query\n",
    "        \"\"\"\n",
    "        if os.path.exists(os.path.join(PICKLES_FOLDER, self.query_file_name + \".pkl\")):\n",
    "            print(\"Loading query from pickle\")\n",
    "            self.query = load_data(self.query_file_name + \".pkl\", PICKLES_FOLDER)\n",
    "        else:\n",
    "            print(f\"Loading query from {self.query_path}\")\n",
    "            if '.csv' in self.query_path:\n",
    "                self.query = pd.read_csv(self.query_path)\n",
    "\n",
    "    def tokenize_corpus(self, drop_text=True):\n",
    "        \"\"\"Tokenize the corpus\n",
    "\n",
    "        Args:\n",
    "            * drop_text (bool): whether to drop the text column or not.\n",
    "        \"\"\"\n",
    "        if self.corpus is None:\n",
    "            self.load_corpus()\n",
    "        self.corpus, self.tokens_list = tokenize_documents(self.corpus_file_name, self.corpus, drop_text)\n",
    "    \n",
    "    def tokenize_query(self, drop_text=True):\n",
    "        \"\"\"Tokenize the query\n",
    "\n",
    "        Args:\n",
    "            * drop_text (bool): whether to drop the text column or not.\n",
    "        \"\"\"\n",
    "        if self.query is None:\n",
    "            self.load_query()\n",
    "        if 'query' in self.query.columns:\n",
    "            self.query.rename(columns={'query': 'text'}, inplace=True)\n",
    "        self.query, self.query_tokens_list = tokenize_documents(self.query_file_name, self.query, drop_text) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4283527",
   "metadata": {
    "papermill": {
     "duration": 0.006039,
     "end_time": "2024-10-17T07:43:44.668397",
     "exception": false,
     "start_time": "2024-10-17T07:43:44.662358",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 6. CorpusBm25 class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f80a7994",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T07:43:44.683288Z",
     "iopub.status.busy": "2024-10-17T07:43:44.682660Z",
     "iopub.status.idle": "2024-10-17T07:43:44.719058Z",
     "shell.execute_reply": "2024-10-17T07:43:44.718024Z"
    },
    "papermill": {
     "duration": 0.046552,
     "end_time": "2024-10-17T07:43:44.721214",
     "exception": false,
     "start_time": "2024-10-17T07:43:44.674662",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CorpusBm25(CorpusBase):\n",
    "    def __init__(self, corpus_path: str, query_path: str):\n",
    "        \"\"\"\n",
    "        Initialize the CorpusBM25 object.\n",
    "\n",
    "        Args:\n",
    "            * corpus_path: str, the path to the corpus file.\n",
    "\n",
    "            * query_path: str, the path to the query file.\n",
    "\n",
    "        Class attributes:\n",
    "            * tf (dict): the term frequency for each term in each document.\n",
    "\n",
    "            * idf (dict): the inverse document frequency for each term.\n",
    "\n",
    "            * df (dict): the document frequency for each term.\n",
    "\n",
    "            * avg_doc_len (float): the average document length.\n",
    "\n",
    "            * doc_len (list): the length of each document in the corpus.\n",
    "\n",
    "            * results (list): the results of the queries.\n",
    "        \"\"\"\n",
    "        super().__init__(corpus_path, query_path)\n",
    "        self.tf = None\n",
    "        self.idf = None\n",
    "        self.df = None\n",
    "        self.avg_doc_len = None\n",
    "        self.doc_len = None\n",
    "        self.results = None\n",
    "\n",
    "    def _compute_df(self):\n",
    "        \"\"\"Compute the document frequency for each term in the corpus (i.e., the number of documents in which the term appears).\n",
    "        \"\"\"\n",
    "        if self.df is None:\n",
    "            if os.path.exists(os.path.join(PICKLES_FOLDER, self.corpus_file_name + \"_df.pkl\")):\n",
    "                print(\"Loading df from pickle\")\n",
    "                self.df = load_data(self.corpus_file_name + \"_df.pkl\", PICKLES_FOLDER)\n",
    "            else:\n",
    "                if self.corpus is None:\n",
    "                    self.load_corpus()\n",
    "                if 'tokens' not in self.corpus.columns:\n",
    "                    self.tokenize_corpus()\n",
    "                print(\"Computing df\")\n",
    "                corpus_tokenized = self.corpus['tokens'].tolist()\n",
    "                self.df = Counter(term for document in corpus_tokenized for term in set(document))\n",
    "                #save_data(self.df, self.corpus_file_name + \"_df.pkl\", PICKLES_FOLDER)   \n",
    "    \n",
    "    def _compute_idf(self):\n",
    "        \"\"\"Compute the inverse document frequency for each term in the corpus.\n",
    "        \"\"\"\n",
    "        if self.idf is None:\n",
    "            if os.path.exists(os.path.join(PICKLES_FOLDER, self.corpus_file_name + \"_idf.pkl\")):\n",
    "                print(\"Loading idf from pickle\")\n",
    "                self.idf = load_data(self.corpus_file_name + \"_idf.pkl\", PICKLES_FOLDER)    \n",
    "            else:\n",
    "                if self.df is None:\n",
    "                    self._compute_df()\n",
    "                print(\"Computing idf\")\n",
    "                num_documents = len(self.corpus)\n",
    "                self.idf = {term: math.log(1 + (num_documents - self.df[term] + 0.5) / (self.df[term] + 0.5)) for term in self.df}\n",
    "                #save_data(self.idf, self.corpus_file_name + \"_idf.pkl\", PICKLES_FOLDER)\n",
    "    \n",
    "    def _compute_tf(self):\n",
    "        \"\"\"Compute the term frequency for each term in each document.\n",
    "        \"\"\"\n",
    "        if self.tf is None:\n",
    "            if os.path.exists(os.path.join(PICKLES_FOLDER, self.corpus_file_name + \"_tf.pkl\")):\n",
    "                print(\"Loading tf from pickle\")\n",
    "                self.tf = load_data(self.corpus_file_name + \"_tf.pkl\", PICKLES_FOLDER)\n",
    "            else:\n",
    "                if self.corpus is None:\n",
    "                    self.load_corpus()\n",
    "                if 'tokens' not in self.corpus.columns:\n",
    "                    self.tokenize_corpus()\n",
    "                print(\"Computing tf\")\n",
    "                corpus_tokenized = self.corpus['tokens'].tolist()\n",
    "                self.tf = {i: dict(Counter(document)) for i, document in enumerate(corpus_tokenized)}\n",
    "                #save_data(self.tf, self.corpus_file_name + \"_tf.pkl\", PICKLES_FOLDER)\n",
    "    \n",
    "    def _compute_doc_len(self):\n",
    "        \"\"\"Compute the length of each document in the corpus.\n",
    "        \"\"\"\n",
    "        if self.doc_len is None or self.avg_doc_len is None:\n",
    "            if os.path.exists(os.path.join(PICKLES_FOLDER, self.corpus_file_name + \"_doc_len.pkl\")) \\\n",
    "                and os.path.exists(os.path.join(PICKLES_FOLDER, self.corpus_file_name + \"_avg_doc_len.pkl\")):\n",
    "                print(\"Loading doc_len from pickle\")\n",
    "                self.doc_len = load_data(self.corpus_file_name + \"_doc_len.pkl\", PICKLES_FOLDER)\n",
    "                self.avg_doc_len = load_data(self.corpus_file_name + \"_avg_doc_len.pkl\", PICKLES_FOLDER)\n",
    "            else:\n",
    "                if self.corpus is None:\n",
    "                    self.load_corpus()\n",
    "                if 'tokens' not in self.corpus.columns:\n",
    "                    self.tokenize_corpus()\n",
    "                print(\"Computing doc_len\")\n",
    "                self.doc_len = [len(document) for document in self.corpus['tokens'].tolist()]\n",
    "                self.avg_doc_len = sum(self.doc_len) / len(self.doc_len)\n",
    "                #save_data(self.doc_len, self.corpus_file_name + \"_doc_len.pkl\", PICKLES_FOLDER)\n",
    "                #save_data(self.avg_doc_len, self.corpus_file_name + \"_avg_doc_len.pkl\", PICKLES_FOLDER)\n",
    "    \n",
    "    def _BM25_search(self,query, docid,relevant_docs, k=10):\n",
    "        \"\"\"Compute BM25 score for all documents in the corpus for a given query and language and return the top-k documents\n",
    "\n",
    "        Args:\n",
    "            * query (list): the tokenized query.\n",
    "\n",
    "            * docid (list): the list of document IDs.\n",
    "\n",
    "            * relevant_docs (list): the list of relevant document IDs.\n",
    "\n",
    "            * k (int): the number of documents to return.\n",
    "\n",
    "        Returns:\n",
    "            * top_doc_ids (list): the list of document IDs with the highest BM25 scores.\n",
    "        \"\"\"\n",
    "\n",
    "        # Calculate scores only for relevant documents\n",
    "        scores = []\n",
    "        for i in relevant_docs:\n",
    "            score = bm25_score(query, i, self.idf, self.tf, self.avg_doc_len, self.doc_len)\n",
    "            scores.append((score, docid[i]))\n",
    "\n",
    "        # Sort and get top-k documents by score\n",
    "        scores.sort(key=lambda x: -x[0])\n",
    "        top_doc_ids = [doc_id for _, doc_id in scores[:k]]\n",
    "        \n",
    "        return top_doc_ids\n",
    "    \n",
    "    def get_results(self):\n",
    "        \"\"\"Get the results of the queries\n",
    "        \"\"\"\n",
    "        self.results = []\n",
    "\n",
    "        #initialize the idf, tf, avg_doc_len, doc_len\n",
    "        print(\"Computing idf, tf, avg_doc_len, doc_len\")\n",
    "        self._compute_df()\n",
    "        self._compute_idf()\n",
    "        self._compute_tf()\n",
    "        self._compute_doc_len()\n",
    "\n",
    "        # Load the queries\n",
    "        if self.query is None:\n",
    "            self.load_query()\n",
    "        if 'tokens' not in self.query.columns:\n",
    "            self.tokenize_query()\n",
    "        \n",
    "        if os.path.exists(os.path.join(PICKLES_FOLDER, self.corpus_file_name + \"_docid.pkl\")):\n",
    "            print(\"Loading docid from pickle\")\n",
    "            docid = load_data(self.corpus_file_name + \"_docid.pkl\", PICKLES_FOLDER)\n",
    "        if os.path.exists(os.path.join(PICKLES_FOLDER, self.corpus_file_name + \"_lang.pkl\")):\n",
    "            print(\"Loading lang from pickle\")\n",
    "            lang = load_data(self.corpus_file_name + \"_lang.pkl\", PICKLES_FOLDER)\n",
    "        else:\n",
    "            #load the corpus\n",
    "            if self.corpus is None:\n",
    "                self.load_corpus()\n",
    "\n",
    "            #extract list of docid, lang and tokenized text from the corpus\n",
    "            docid = self.corpus['docid'].tolist()\n",
    "            lang = self.corpus['lang'].tolist()\n",
    "\n",
    "        #extract list of tokenized text and lang from the test queries\n",
    "        list_test_queries = self.query[\"tokens\"].tolist()\n",
    "        list_lang_test_queries = self.query[\"lang\"].tolist()\n",
    "\n",
    "        # Create a dictionary with the relevant documents for each language\n",
    "        langs = set(lang)\n",
    "        dict_relevant_docs = {l: [i for i in range(len(docid)) if lang[i] == l] for l in langs}\n",
    "\n",
    "        # Loop over each query and calculate the BM25 scores\n",
    "        start = time()\n",
    "        for idx, query in tqdm(enumerate(list_test_queries), total=len(list_test_queries), desc=\"Calculating BM25 scores\"):\n",
    "            query_lang = list_lang_test_queries[idx]  # Get the language for the current query\n",
    "            \n",
    "            # Get the top 10 documents for the current query\n",
    "            relevant_docs = dict_relevant_docs[query_lang]\n",
    "            top_docs = self._BM25_search(query, docid,relevant_docs, k=10)\n",
    "            \n",
    "            # Append the result as a dictionary\n",
    "            self.results.append({\n",
    "                'id': idx,  # You may replace idx with actual query ID if available\n",
    "                'docids': top_docs\n",
    "            })  \n",
    "        end = time()\n",
    "        print(f\"Time taken to calculate BM25 scores: {end - start:.2f} seconds\") \n",
    "\n",
    "    def create_submission(self, output_path: str):\n",
    "        \"\"\" Create a submission file for the BM25 model.\n",
    "\n",
    "        Args:\n",
    "            * output_path (str): the path to the output file.\n",
    "        \"\"\"\n",
    "        if self.results is None:\n",
    "            self.get_results()\n",
    "        results_df = pd.DataFrame(self.results)\n",
    "        results_df.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbd03f3",
   "metadata": {
    "papermill": {
     "duration": 0.006069,
     "end_time": "2024-10-17T07:43:44.733641",
     "exception": false,
     "start_time": "2024-10-17T07:43:44.727572",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 7. Final Part: Computing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10fbc0a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T07:43:44.747832Z",
     "iopub.status.busy": "2024-10-17T07:43:44.747395Z",
     "iopub.status.idle": "2024-10-17T07:43:44.752251Z",
     "shell.execute_reply": "2024-10-17T07:43:44.751228Z"
    },
    "papermill": {
     "duration": 0.014536,
     "end_time": "2024-10-17T07:43:44.754466",
     "exception": false,
     "start_time": "2024-10-17T07:43:44.739930",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "documents = CorpusBm25(CORPUS, QUERIES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d370e0d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T07:43:44.768287Z",
     "iopub.status.busy": "2024-10-17T07:43:44.767871Z",
     "iopub.status.idle": "2024-10-17T08:06:56.706794Z",
     "shell.execute_reply": "2024-10-17T08:06:56.705592Z"
    },
    "papermill": {
     "duration": 1391.949464,
     "end_time": "2024-10-17T08:06:56.710003",
     "exception": false,
     "start_time": "2024-10-17T07:43:44.760539",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing idf, tf, avg_doc_len, doc_len\n",
      "Loading df from pickle\n",
      "Loading idf from pickle\n",
      "Loading tf from pickle\n",
      "Loading doc_len from pickle\n",
      "Loading query from /kaggle/input/dis-project-1-document-retrieval/test.csv\n",
      "Tokenizing corpus\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:01<00:00, 1751.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading docid from pickle\n",
      "Loading lang from pickle\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating BM25 scores: 100%|██████████| 2000/2000 [21:19<00:00,  1.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to calculate BM25 scores: 1279.43 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "documents.create_submission(output_path='submission_bm25_v.csv')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 9635715,
     "sourceId": 85316,
     "sourceType": "competition"
    },
    {
     "datasetId": 5892189,
     "sourceId": 9647636,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5892229,
     "sourceId": 9647682,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 201329707,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "DIS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1402.490469,
   "end_time": "2024-10-17T08:07:00.879054",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-10-17T07:43:38.388585",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
