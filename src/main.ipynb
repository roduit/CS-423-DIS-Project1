{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# -*- authors : Vincent Roduit -*-\n",
    "# -*- date : 2024-09-30 -*-\n",
    "# -*- Last revision: 2024-09-30 by Vincent Roduit -*-\n",
    "# -*- python version : 3.9.19 -*-\n",
    "# -*- Description: Constants used in the code *-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> CS - 423: Distributed Information Systems </center>\n",
    "## <center> Ecole Polytechnique Fédérale de Lausanne </center>\n",
    "### <center>Project 1: Document Retrieval </center>\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "import numpy as np  \n",
    "\n",
    "#import files\n",
    "from corpus_word2vec import CorpusWord2Vec\n",
    "from constants import *\n",
    "from corpus_bm25 import CorpusBm25\n",
    "from scores import recall_at_k\n",
    "\n",
    "# automatically reload  the module\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Data exploration\n",
    "This first section will explore the corpus and the queries to have a better understanding of the data we have to deal with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Retrieve documents using Word2Vec as word embedding method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = CorpusWord2Vec(CORPUS, QUERIES)\n",
    "documents.create_submission(output_path=os.path.join(SUBMISSIONS_FOLDER, 'submission_test.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Retrieve documents using TF-IDF method and BM25 ranking score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing idf, tf, avg_doc_len, doc_len\n",
      "Loading corpus from pickle\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 268022/268022 [01:09<00:00, 3835.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing df\n",
      "Computing idf\n",
      "Computing tf\n",
      "Computing doc_len\n",
      "Computing length_norm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing inverted index: 100%|██████████| 268022/268022 [01:34<00:00, 2843.31it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading query from ../data/train.csv\n",
      "Tokenizing corpus\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21875/21875 [00:02<00:00, 9524.85it/s] \n",
      "Calculating BM25 scores: 100%|██████████| 21875/21875 [32:59<00:00, 11.05it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to calculate BM25 scores: 1983.80 seconds\n"
     ]
    }
   ],
   "source": [
    "documents = CorpusBm25(CORPUS, QUERIES_TRAIN)\n",
    "documents.create_submission(output_path=os.path.join(SUBMISSIONS_FOLDER, 'submission_train.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Evaluate performances\n",
    "In this section we will evaluate the performances of the BM25 ranking on the train set. The evaluation will be done using the recall at 10 function, which is the same metric as the one provided on the Kaggle competition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Overall performance\n",
    "In this section, the performance will be done regardless of the language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame(documents.results)\n",
    "df_queries = pd.DataFrame(documents.query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = df_results.merge(df_queries, left_index=True, right_index=True)\n",
    "predictions = df_results['docids'].tolist()\n",
    "ground_truth = df_results['positive_docs'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall@10: 0.26765714285714287\n"
     ]
    }
   ],
   "source": [
    "recalls = []\n",
    "for i in range(len(predictions)):\n",
    "    pred = predictions[i]\n",
    "    gt = ground_truth[i]\n",
    "    rec = recall_at_k(pred, gt, 10)\n",
    "    recalls.append(rec)\n",
    "\n",
    "print(f\"Recall@10: {np.mean(recalls)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 Performance per langage\n",
    "This section will focuses on looking at the performances of the BM25 by separating the different langages to see any differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall@10 for en: 0.0559\n",
      "Recall@10 for fr: 0.22139303482587064\n",
      "Recall@10 for de: 0.37357877639415266\n",
      "Recall@10 for es: 0.5248447204968945\n",
      "Recall@10 for it: 0.33844723384472336\n",
      "Recall@10 for ko: 0.6014558689717925\n",
      "Recall@10 for ar: 0.5597138139790864\n"
     ]
    }
   ],
   "source": [
    "langs = df_queries['lang'].unique()\n",
    "\n",
    "recalls = {}\n",
    "for lang in langs:\n",
    "    df_lang = df_results[df_results['lang'] == lang]\n",
    "    predictions = df_lang['docids'].tolist()\n",
    "    ground_truth = df_lang['positive_docs'].tolist()\n",
    "    recalls[lang] = []\n",
    "    for i in range(len(predictions)):\n",
    "        pred = predictions[i]\n",
    "        gt = ground_truth[i]\n",
    "        rec = recall_at_k(pred, gt, 10)\n",
    "        recalls[lang].append(rec)\n",
    "    \n",
    "    print(f\"Recall@10 for {lang}: {np.mean(recalls[lang])}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DIS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
