{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# -*- authors : Vincent Roduit, Yann Cretton, Fabio Palmisano -*-\n",
    "# -*- date : 2024-09-30 -*-\n",
    "# -*- Last revision: 2024-09-30 by Vincent Roduit -*-\n",
    "# -*- python version : 3.9.19 -*-\n",
    "# -*- Description: Notebook that summarizes the results -*-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> CS - 423: Distributed Information Systems </center>\n",
    "## <center> Ecole Polytechnique Fédérale de Lausanne </center>\n",
    "### <center>Project 1: Document Retrieval </center>\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "import numpy as np  \n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "#import files\n",
    "from corpus_word2vec import CorpusWord2Vec\n",
    "from constants import *\n",
    "from corpus_bm25 import CorpusBm25\n",
    "from scores import recall_at_k, evaluate_recall_at_k, evaluate_recall_at_k_per_lang\n",
    "\n",
    "# automatically reload  the module\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Data exploration\n",
    "This first section will explore the corpus and the queries to have a better understanding of the data we have to deal with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1 corpus.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = pd.read_json(CORPUS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_per_lang = corpus.groupby('lang')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_per_lang['docid'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.2 train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(QUERIES_TRAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_per_lang = df_train.groupby('lang')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_per_lang['query_id'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.3 dev.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dev = pd.read_csv(DEV)\n",
    "df_dev.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dev_per_lang = df_dev.groupby('lang')\n",
    "df_dev_per_lang['query_id'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.4 test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(QUERIES)\n",
    "df_test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_per_lang = df_test.groupby('lang')\n",
    "df_test_per_lang['query_id'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Retrieve documents using Word2Vec as word embedding method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train queries\n",
    "documents = CorpusWord2Vec(CORPUS, QUERIES_TRAIN)\n",
    "\n",
    "documents.create_submission(output_path=os.path.join(SUBMISSIONS_FOLDER, 'submission_word2vec.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dev queries\n",
    "documents = CorpusWord2Vec(CORPUS, DEV)\n",
    "documents.create_submission(output_path=os.path.join(SUBMISSIONS_FOLDER, 'submission_word2vec_dev.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1 Evaluate performances\n",
    "In this section we will evaluate the performances of the Word2Vec ranking on the train set. The evaluation will be done using the recall at 10 function, which is the same metric as the one provided on the Kaggle competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall@k for train -------------------\n",
      "Recall@10: 0.20\n",
      "recall@k for dev -------------------\n",
      "Recall@10: 0.28\n"
     ]
    }
   ],
   "source": [
    "print('recall@k for train -------------------')\n",
    "_ =evaluate_recall_at_k(submission_name='submission_word2vec.csv', queries_path=QUERIES_TRAIN)\n",
    "print('recall@k for dev -------------------')\n",
    "_ = evaluate_recall_at_k(submission_name='submission_word2vec_dev.csv', queries_path=DEV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall@k for train -------------------\n",
      "Recall@10 for en: 0.19\n",
      "Recall@10 for fr: 0.20\n",
      "Recall@10 for de: 0.15\n",
      "Recall@10 for es: 0.29\n",
      "Recall@10 for it: 0.21\n",
      "Recall@10 for ko: 0.17\n",
      "Recall@10 for ar: 0.27\n",
      "recall@k for dev -------------------\n",
      "Recall@10 for en: 0.17\n",
      "Recall@10 for fr: 0.39\n",
      "Recall@10 for de: 0.22\n",
      "Recall@10 for es: 0.39\n",
      "Recall@10 for it: 0.33\n",
      "Recall@10 for ko: 0.14\n",
      "Recall@10 for ar: 0.29\n"
     ]
    }
   ],
   "source": [
    "print('recall@k for train -------------------')\n",
    "_ = evaluate_recall_at_k_per_lang(submission_name='submission_word2vec.csv', queries_path=QUERIES_TRAIN)\n",
    "print('recall@k for dev -------------------')\n",
    "_ = evaluate_recall_at_k_per_lang(submission_name='submission_word2vec_dev.csv', queries_path=DEV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Retrieve documents using TF-IDF method and BM25 ranking score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Normal BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train queries\n",
    "documents = CorpusBm25(CORPUS, QUERIES_TRAIN, filter=False)\n",
    "documents.create_submission(output_path=os.path.join(SUBMISSIONS_FOLDER, 'submission_train_bm25_normal.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dev queries\n",
    "documents = CorpusBm25(CORPUS, DEV, filter=False)\n",
    "documents.create_submission(output_path=os.path.join(SUBMISSIONS_FOLDER, 'submission_dev_bm25_normal.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Evaluate performances\n",
    "In this section we will evaluate the performances of the BM25 ranking on the train set. The evaluation will be done using the recall at 10 function, which is the same metric as the one provided on the Kaggle competition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overall performance\n",
    "In this section, the performance will be done regardless of the language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall@k for train -------------------\n",
      "Recall@10: 0.67\n",
      "recall@k for dev -------------------\n",
      "Recall@10: 0.78\n"
     ]
    }
   ],
   "source": [
    "print('recall@k for train -------------------')\n",
    "_= evaluate_recall_at_k(submission_name='submission_train_bm25_normal.csv', queries_path=QUERIES_TRAIN)\n",
    "print('recall@k for dev -------------------')\n",
    "_ = evaluate_recall_at_k(submission_name='submission_dev_bm25_normal.csv', queries_path=DEV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance per langage\n",
    "This section will focuses on looking at the performances of the BM25 by separating the different langages to see any differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall@k for train -------------------\n",
      "Recall@10 for en: 0.84\n",
      "Recall@10 for fr: 0.41\n",
      "Recall@10 for de: 0.39\n",
      "Recall@10 for es: 0.65\n",
      "Recall@10 for it: 0.49\n",
      "Recall@10 for ko: 0.60\n",
      "Recall@10 for ar: 0.56\n",
      "recall@k for dev -------------------\n",
      "Recall@10 for en: 0.76\n",
      "Recall@10 for fr: 0.90\n",
      "Recall@10 for de: 0.69\n",
      "Recall@10 for es: 0.93\n",
      "Recall@10 for it: 0.80\n",
      "Recall@10 for ko: 0.63\n",
      "Recall@10 for ar: 0.74\n"
     ]
    }
   ],
   "source": [
    "print('recall@k for train -------------------')\n",
    "_ = evaluate_recall_at_k_per_lang(submission_name='submission_train_bm25_normal.csv', queries_path=QUERIES_TRAIN)\n",
    "print('recall@k for dev -------------------')\n",
    "_ = evaluate_recall_at_k_per_lang(submission_name='submission_dev_bm25_normal.csv', queries_path=DEV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Filtered BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train\n",
    "documents = CorpusBm25(CORPUS, QUERIES_TRAIN, filter=True, filt_docs=10e3)\n",
    "documents.create_submission(output_path=os.path.join(SUBMISSIONS_FOLDER, 'submission_train_bm25_filt_2e5.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dev\n",
    "documents = CorpusBm25(CORPUS, DEV, filter=True, filt_docs=10e3)\n",
    "documents.create_submission(output_path=os.path.join(SUBMISSIONS_FOLDER, 'submission_dev_bm25_filt_2e5.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Evaluate performances\n",
    "In this section we will evaluate the performances of the BM25 ranking with filtered document on the train set. The evaluation will be done using the recall at 10 function, which is the same metric as the one provided on the Kaggle competition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overall performance\n",
    "In this section, the performance will be done regardless of the language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall@k for train -------------------\n",
      "Recall@10: 0.27\n",
      "recall@k for dev -------------------\n",
      "Recall@10: 0.80\n"
     ]
    }
   ],
   "source": [
    "print('recall@k for train -------------------')\n",
    "_ = evaluate_recall_at_k(submission_name='submission_train_bm25_filt_2e5.csv', queries_path=QUERIES_TRAIN)\n",
    "print('recall@k for dev -------------------')\n",
    "_ = evaluate_recall_at_k(submission_name='submission_dev_bm25_filt_2e5.csv', queries_path=DEV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance per langage\n",
    "This section will focuses on looking at the performances of the BM25 by separating the different langages to see any differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall@k for train -------------------\n",
      "Recall@10 for en: 0.06\n",
      "Recall@10 for fr: 0.22\n",
      "Recall@10 for de: 0.38\n",
      "Recall@10 for es: 0.52\n",
      "Recall@10 for it: 0.34\n",
      "Recall@10 for ko: 0.60\n",
      "Recall@10 for ar: 0.56\n",
      "recall@k for dev -------------------\n",
      "Recall@10 for en: 0.88\n",
      "Recall@10 for fr: 0.90\n",
      "Recall@10 for de: 0.69\n",
      "Recall@10 for es: 0.93\n",
      "Recall@10 for it: 0.80\n",
      "Recall@10 for ko: 0.63\n",
      "Recall@10 for ar: 0.74\n"
     ]
    }
   ],
   "source": [
    "print('recall@k for train -------------------')\n",
    "_ = evaluate_recall_at_k_per_lang(submission_name='submission_train_bm25_filt_2e5.csv', queries_path=QUERIES_TRAIN)\n",
    "print('recall@k for dev -------------------')\n",
    "_ = evaluate_recall_at_k_per_lang(submission_name='submission_dev_bm25_filt_2e5.csv', queries_path=DEV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Fine-tuning model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Compare filtering effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt_docs = [100, 500, 1000, 5000, 10000, 15000]\n",
    "recalls = []\n",
    "recalls_per_lang = []\n",
    "timestamps = []\n",
    "\n",
    "for filt in filt_docs:\n",
    "    print(f'Filt: {filt}')\n",
    "    documents = CorpusBm25(CORPUS, DEV, filter=True, filt_docs=filt, verbose=False)\n",
    "    documents.create_submission(output_path=os.path.join(SUBMISSIONS_FOLDER, f'submission_train_bm25_filt_{filt}.csv'))\n",
    "    recall = evaluate_recall_at_k(submission_name=f'submission_train_bm25_filt_{filt}.csv', queries_path=DEV)\n",
    "    recalls.append(recall)\n",
    "    recall = evaluate_recall_at_k_per_lang(submission_name=f'submission_train_bm25_filt_{filt}.csv', queries_path=DEV)\n",
    "    recalls_per_lang.append(recall)\n",
    "    timestamps.append(documents.time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(filt_docs, recalls)\n",
    "plt.xlabel('Number of documents')\n",
    "plt.ylabel('Recall@10')\n",
    "plt.title('Recall@10 vs Number of documents')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot recall per language\n",
    "for lang in recalls_per_lang[0]:\n",
    "    plt.plot(filt_docs, [recalls_per_lang[k][lang] for k in range(len(filt_docs))], label=lang)\n",
    "plt.xlabel('Number of documents')\n",
    "plt.ylabel('Recall@10')\n",
    "plt.title('Recall@10 vs Number of documents per language')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot time (yleft) and performance (yright)\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "color = 'tab:red'\n",
    "ax1.set_xlabel('Number of documents')\n",
    "ax1.set_ylabel('Recall@10', color=color)\n",
    "ax1.plot(filt_docs, recalls, color=color)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('Time', color=color)\n",
    "ax2.plot(filt_docs, timestamps, color=color)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.title('Recall@10 and Time vs Number of documents')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning b and k1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# study the effect of k1 and b\n",
    "k1_values = [0.1, 0.5, 1, 1.5, 2, 2.5, 3]\n",
    "b_values = [0.1, 0.5, 0.75, 0.9, 0.95, 0.99]\n",
    "recalls = np.zeros((len(k1_values), len(b_values)))\n",
    "recalls_per_lang = {lang: np.zeros((len(k1_values), len(b_values))) for lang in STOP_WORDS.keys()}\n",
    "timestamps = np.zeros((len(k1_values), len(b_values)))\n",
    "\n",
    "\n",
    "for i, k1 in enumerate(k1_values):\n",
    "    for j, b in enumerate(b_values):\n",
    "        print(f'k1: {k1}, b: {b}')\n",
    "        documents = CorpusBm25(CORPUS, DEV, filter=True, filt_docs=10000, k1=k1, b=b, verbose=False)\n",
    "        documents.create_submission(output_path=os.path.join(SUBMISSIONS_FOLDER, f'submission_train_bm25_k1_{k1}_b_{b}.csv'))\n",
    "        recall = evaluate_recall_at_k(submission_name=f'submission_train_bm25_k1_{k1}_b_{b}.csv', queries_path=DEV)\n",
    "        recalls[i, j] = recall\n",
    "        timestamps[i, j] = documents.time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot results given k1 and b with values inside the case\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(recalls, cmap='hot', interpolation='nearest')\n",
    "plt.xticks(range(len(b_values)), b_values)\n",
    "plt.yticks(range(len(k1_values)), k1_values)\n",
    "plt.xlabel('b')\n",
    "plt.ylabel('k1')\n",
    "for i in range(len(k1_values)):\n",
    "    for j in range(len(b_values)):\n",
    "        plt.text(j, i, f'{recalls[i, j]:.3f}', ha='center', va='center', color='black')\n",
    "plt.colorbar()\n",
    "plt.title('Recall@10 vs k1 and b')\n",
    "plt.show()\n",
    "\n",
    "#find the best k1 and b\n",
    "best_k1 = k1_values[np.unravel_index(np.argmax(recalls), recalls.shape)[0]]\n",
    "best_b = b_values[np.unravel_index(np.argmax(recalls), recalls.shape)[1]]\n",
    "\n",
    "print(f'Best k1: {best_k1}, Best b: {best_b}')\n",
    "print(f'Best recall: {np.max(recalls)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Compute the best prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing idf, tf, avg_doc_len, doc_len\n",
      "Loading df from pickle\n",
      "Loading idf from pickle\n",
      "Loading tf from pickle\n",
      "Loading doc_len from pickle\n",
      "Computing length_norm\n",
      "Loading inverted index from pickle\n",
      "Loading docid from pickle\n",
      "Loading lang from pickle\n",
      "Loading query from ../data/dev.csv\n",
      "Loading tokenized corpus from pickle\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating BM25 scores: 100%|██████████| 1400/1400 [01:19<00:00, 17.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to process queries and compute BM25 scores: 1 min 19 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Compute the predictions with the best k1 and b for the dev set\n",
    "best_k1 = 2\n",
    "best_b = 0.9\n",
    "documents = CorpusBm25(CORPUS, DEV, filter=True, k1=best_k1, b=best_b)\n",
    "documents.create_submission(output_path=os.path.join(SUBMISSIONS_FOLDER, f'submission_dev_bm25_k1_{best_k1}_b_{best_b}.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall@k for dev -------------------\n",
      "Recall@10: 0.80\n"
     ]
    }
   ],
   "source": [
    "print('recall@k for dev -------------------')\n",
    "_ = evaluate_recall_at_k(submission_name=f'submission_dev_bm25_k1_{best_k1}_b_{best_b}.csv', queries_path=DEV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall@k for dev -------------------\n",
      "Recall@10 for en: 0.87\n",
      "Recall@10 for fr: 0.90\n",
      "Recall@10 for de: 0.70\n",
      "Recall@10 for es: 0.94\n",
      "Recall@10 for it: 0.81\n",
      "Recall@10 for ko: 0.62\n",
      "Recall@10 for ar: 0.74\n"
     ]
    }
   ],
   "source": [
    "print('recall@k for dev -------------------')\n",
    "_ = evaluate_recall_at_k_per_lang(submission_name=f'submission_dev_bm25_k1_{best_k1}_b_{best_b}.csv', queries_path=DEV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing idf, tf, avg_doc_len, doc_len\n",
      "Loading df from pickle\n",
      "Loading idf from pickle\n",
      "Loading tf from pickle\n",
      "Loading doc_len from pickle\n",
      "Computing length_norm\n",
      "Loading inverted index from pickle\n",
      "Loading docid from pickle\n",
      "Loading lang from pickle\n",
      "Loading query from ../data/test.csv\n",
      "Loading tokenized corpus from pickle\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating BM25 scores: 100%|██████████| 2000/2000 [02:40<00:00, 12.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to process queries and compute BM25 scores: 2 min 40 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Compute the test set with the best k1 and b\n",
    "documents = CorpusBm25(CORPUS, QUERIES, filter=True, k1=best_k1, b=best_b)\n",
    "documents.create_submission(output_path=os.path.join(SUBMISSIONS_FOLDER, f'possibly_best.csv'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DIS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
