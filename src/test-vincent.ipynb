{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# -*- authors : Vincent Roduit -*-\n",
    "# -*- date : 2024-09-30 -*-\n",
    "# -*- Last revision: 2024-09-30 by Vincent Roduit -*-\n",
    "# -*- python version : 3.9.19 -*-\n",
    "# -*- Description: Constants used in the code *-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> CS - 423: Distributed Information Systems </center>\n",
    "## <center> Ecole Polytechnique Fédérale de Lausanne </center>\n",
    "### <center>Project 1: Document Retrieval </center>\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "import pickle as pkl\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "import string\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from collections import Counter\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# automatically reload the module\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Declaring constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the data folder\n",
    "if os.path.exists(\"../data\"):\n",
    "    DATA_FOLDER = \"../data\"\n",
    "else:\n",
    "    # create the data folder\n",
    "    os.mkdir(\"../data\")\n",
    "\n",
    "CORPUS = os.path.join(DATA_FOLDER, \"corpus\", \"corpus.json\")\n",
    "CORPUS_PKL = os.path.join(DATA_FOLDER, \"pickles\", \"corpus.pkl\")\n",
    "CORPUS_REDUCED_PKL = os.path.join(DATA_FOLDER, \"pickles\", \"corpus_reduced.pkl\")\n",
    "\n",
    "STOP_WORDS = {\n",
    "    \"en\": set(stopwords.words('english')),\n",
    "    \"fr\": set(stopwords.words('french')),\n",
    "    \"de\": set(stopwords.words('german')),\n",
    "    \"es\": set(stopwords.words('spanish')),\n",
    "    \"it\": set(stopwords.words('italian')),\n",
    "    \"ko\": set(stopwords.words('korean')),\n",
    "    \"ar\": set(stopwords.words('arabic')),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(data: any, file_name: str, folder: str = os.path.join(DATA_FOLDER, \"pickles\")):\n",
    "    \"\"\"\n",
    "    Save the data to a file\n",
    "    Args:\n",
    "\n",
    "    * data (any): the data to save\n",
    "\n",
    "    * file_name (str): the name of the file\n",
    "\n",
    "    * folder (str): the folder where to save the file\n",
    "    \"\"\"\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "\n",
    "    file_path = os.path.join(folder, file_name)\n",
    "\n",
    "    with open(file_path, 'wb') as handle:\n",
    "        pkl.dump(data, handle)\n",
    "\n",
    "def load_data(file_name: str, folder: str = os.path.join(DATA_FOLDER, \"pickles\")) -> any:\n",
    "    \"\"\"\n",
    "    Load the data from a file\n",
    "    Args:\n",
    "\n",
    "    * file_name (str): the name of the file\n",
    "\n",
    "    * folder (str): the folder where to save the file\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    * any: the data\n",
    "    \"\"\"\n",
    "    file_path = os.path.join(folder, file_name)\n",
    "\n",
    "    with open(file_path, 'rb') as handle:\n",
    "        data = pkl.load(handle)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the corpus from the json file\n"
     ]
    }
   ],
   "source": [
    "# load the corpus\n",
    "if os.path.exists(CORPUS_PKL):\n",
    "    print(\"Loading the corpus from the pickle file\")\n",
    "    corpus = load_data(\"corpus.pkl\")\n",
    "else:\n",
    "    print(\"Loading the corpus from the json file\")\n",
    "    corpus = pd.read_json(CORPUS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "division = len(corpus) // 3\n",
    "\n",
    "df_corpus_vincent = corpus[:division].copy()\n",
    "# df_corpus_fabio = corpus[division:2*division].copy()\n",
    "# df_corpus_yann = corpus[2*division:].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize objects outside the function to avoid re-initialization overhead\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def tokenize(text, lang=\"en\"):\n",
    "    \"\"\"\n",
    "    Tokenizes and stems the input text efficiently.\n",
    "    \n",
    "    :param text: str, input text to process\n",
    "    :return: list, tokenized and stemmed words\n",
    "    \"\"\"\n",
    "    \n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    \n",
    "    # Combine stemming and stopword filtering into one pass for efficiency\n",
    "    return [stemmer.stem(word.lower()) for word in tokens if word.lower() not in STOP_WORDS[lang] or stemmer.stem(word.lower()) not in STOP_WORDS[lang]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 59.00it/s]\n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas()\n",
    "\n",
    "# Apply the tokenization\n",
    "# df_corpus_fabio['tokenized'] = df_corpus_fabio.progress_apply(lambda row: tokenize(row['text'], lang=row['lang']), axis=1)\n",
    "# save_data(df_corpus_fabio, \"df_corpus_fabio.pkl\")\n",
    "\n",
    "df_corpus_vincent['tokenized'] = df_corpus_vincent.progress_apply(lambda row: tokenize(row['text'], lang=row['lang']), axis=1)\n",
    "save_data(df_corpus_vincent, \"df_corpus_vincent.pkl\")\n",
    "\n",
    "# df_corpus_yann['tokenized'] = df_corpus_yann.progress_apply(lambda row: tokenize(row['text'], lang=row['lang']), axis=1)\n",
    "# save_data(df_corpus_yann, \"df_corpus_yann.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens_list_fabio = df_corpus_fabio['tokenized'].tolist()\n",
    "# save_data(tokens_list_fabio, \"tokens_list_fabio.pkl\")\n",
    "\n",
    "tokens_list_vincent = df_corpus_vincent['tokenized'].tolist()\n",
    "save_data(tokens_list_vincent, \"tokens_list_vincent.pkl\")\n",
    "\n",
    "# tokens_list_yann = df_corpus_yann['tokenized'].tolist()\n",
    "# save_data(tokens_list_yann, \"tokens_list_yann.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Create the TF-IDF matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for the TF/IDF implementation\n",
    "def idf_values(vocabulary, documents):\n",
    "    \"\"\"\n",
    "    It computes IDF scores, storing idf values in a dictionary.\n",
    "    \n",
    "    :param vocabulary: list of str, with the unique tokens of the vocabulary.\n",
    "    :param documents: list of lists of str, with tokenized sentences.\n",
    "    :return: dict with the idf values for each vocabulary word.\n",
    "    \"\"\"\n",
    "    idf = {}\n",
    "    num_documents = len(documents)\n",
    "    \n",
    "    # Adding tqdm progress bar for the loop\n",
    "    for term in tqdm(vocabulary, desc=\"Calculating IDF values\"):\n",
    "        idf[term] = math.log(num_documents / sum([1 for d in documents if term in d]), math.e)\n",
    "    \n",
    "    return idf\n",
    "\n",
    "def vectorize(document, vocabulary, idf):\n",
    "    \"\"\"\n",
    "    It generates the vector for an input document (with normalization).\n",
    "    \n",
    "    :param document: list of str with the tokenized documents.\n",
    "    :param vocabulary: list of str, with the unique tokens of the vocabulary.\n",
    "    :param idf: dict with the idf values for each vocabulary word.\n",
    "    :return: list of floats\n",
    "    \"\"\"\n",
    "    vector = [0]*len(vocabulary)\n",
    "    counts = Counter(document)\n",
    "    max_count = counts.most_common(1)[0][1]\n",
    "    for i,term in enumerate(vocabulary):\n",
    "        vector[i] = idf[term] * counts[term] / max_count\n",
    "    return vector\n",
    "\n",
    "def cosine_similarity(v1,v2):\n",
    "    \"\"\"\n",
    "    It computes cosine similarity.\n",
    "    \n",
    "    :param v1: list of floats, with the vector of a document.\n",
    "    :param v2: list of floats, with the vector of a document.\n",
    "    :return: float\n",
    "    \"\"\"\n",
    "    \n",
    "    sumxx, sumxy, sumyy = 0, 0, 0\n",
    "    for i in range(len(v1)):\n",
    "        x = v1[i]; y = v2[i]\n",
    "        sumxx += x*x\n",
    "        sumyy += y*y\n",
    "        sumxy += x*y\n",
    "    if sumxy == 0:\n",
    "            result = 0\n",
    "    else:\n",
    "            result =  sumxy / math.sqrt(sumxx*sumyy)\n",
    "    return result\n",
    "\n",
    "def search_vec(query, topk, corpus, idf, vocabulary, document_vectors):\n",
    "    \"\"\"\n",
    "    It computes the search result (get the topk documents).\n",
    "    \n",
    "    :param query: str\n",
    "    :param topk: int\n",
    "    \"\"\"\n",
    "    q = query.split()\n",
    "    q = [stemmer.stem(w) for w in q]\n",
    "    query_vector = vectorize(q, vocabulary, idf)\n",
    "    scores = [[cosine_similarity(query_vector, document_vectors[d]), d] for d in range(len(corpus))]\n",
    "    scores.sort(key=lambda x: -x[0])\n",
    "    doc_ids = []\n",
    "    for i in range(topk):\n",
    "        doc_ids.append(scores[i][1])\n",
    "\n",
    "    return doc_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def term_frequency(corpus):\n",
    "    \"\"\"\n",
    "    It computes the term frequency for each term in the corpus.\n",
    "    \n",
    "    :param corpus: list of lists of str, with the tokenized documents.\n",
    "    :return: dict with the term frequency for each term.\n",
    "    \"\"\"\n",
    "    term_freq = {}\n",
    "    for document in corpus:\n",
    "        for term in document:\n",
    "            if term in term_freq:\n",
    "                term_freq[term] += 1\n",
    "            else:\n",
    "                term_freq[term] = 1\n",
    "    return term_freq"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DIS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
