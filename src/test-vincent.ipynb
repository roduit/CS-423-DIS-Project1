{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# -*- authors : Vincent Roduit -*-\n",
    "# -*- date : 2024-09-30 -*-\n",
    "# -*- Last revision: 2024-09-30 by Vincent Roduit -*-\n",
    "# -*- python version : 3.9.19 -*-\n",
    "# -*- Description: Constants used in the code *-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> CS - 423: Distributed Information Systems </center>\n",
    "## <center> Ecole Polytechnique Fédérale de Lausanne </center>\n",
    "### <center>Project 1: Document Retrieval </center>\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "import pickle as pkl\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "import string\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from collections import Counter\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from gensim.models import Word2Vec\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "from time import time\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cores = multiprocessing.cpu_count()\n",
    "\n",
    "#import files\n",
    "from constants import *\n",
    "from utils import *\n",
    "from corpus import *\n",
    "\n",
    "# automatically reload the module\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading corpus from pickle\n"
     ]
    }
   ],
   "source": [
    "documents = Corpus(corpus_path=CORPUS)\n",
    "documents.load_corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(doc, stopwords, w2v_model) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Identify the vector values for each word in the given document\n",
    "        :param doc:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        doc = doc.lower()\n",
    "        words = [w for w in doc.split(\" \") if w not in stopwords]\n",
    "        word_vecs = []\n",
    "        for word in words:\n",
    "            try:\n",
    "                vec = w2v_model[word]\n",
    "                word_vecs.append(vec)\n",
    "            except KeyError:\n",
    "                # Ignore, if the word doesn't exist in the vocabulary\n",
    "                pass\n",
    "        if not word_vecs:\n",
    "            # If empty - return zeros\n",
    "            return np.zeros(w2v_model.vector_size)\n",
    "        vector = np.mean(word_vecs, axis=0)\n",
    "        return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 268022/268022 [25:44<00:00, 173.54it/s]\n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas()\n",
    "corpus['vectors'] = corpus.progress_apply(lambda x: vectorize(x['text'], STOP_WORDS['en'], w2v_model), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = np.array(corpus['vectors'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(268022, 300)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize objects outside the function to avoid re-initialization overhead\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def tokenize(text, lang=\"en\"):\n",
    "    \"\"\"\n",
    "    Tokenizes and stems the input text efficiently.\n",
    "    \n",
    "    :param text: str, input text to process\n",
    "    :return: list, tokenized and stemmed words\n",
    "    \"\"\"\n",
    "    \n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    \n",
    "    # Combine stemming and stopword filtering into one pass for efficiency\n",
    "    return [stemmer.stem(word.lower()) for word in tokens if word.lower() not in STOP_WORDS[lang] or stemmer.stem(word.lower()) not in STOP_WORDS[lang]]\n",
    "\n",
    "def get_vectors(words, model):\n",
    "    \"\"\"\n",
    "    Get the vectors of the words from the model\n",
    "    \n",
    "    :param words: list, list of words\n",
    "    :param model: Word2Vec, the Word2Vec model\n",
    "    :return: list, list of vectors\n",
    "    \"\"\"\n",
    "    vectors = []\n",
    "    for word in words:\n",
    "        try:\n",
    "            vectors.append(model.wv[word])\n",
    "        except KeyError:\n",
    "            pass\n",
    "    return np.mean(vectors, axis=0) if vectors else np.zeros(model.vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec model already exists, loading it...\n"
     ]
    }
   ],
   "source": [
    "def create_word2vec_model(model_name, tokens_list=None,min_count=1, window=5, vector_size=100, workers=cores-1, epochs=100):\n",
    "    \"\"\"\n",
    "    Create a Word2Vec model from the tokens list and save it to a file.\n",
    "    \n",
    "    :param tokens_list: list, list of tokenized words\n",
    "    :param model_name: str, name of the model file\n",
    "    :param min_count: int, minimum number of occurrences of a word to be included in the model\n",
    "    :param window: int, maximum distance between the current and predicted word within a sentence\n",
    "    :param vector_size: int, dimensionality of the word vectors\n",
    "    :param workers: int, number of worker threads to train the model\n",
    "    :param epochs: int, number of iterations over the corpus\n",
    "    \"\"\"\n",
    "    \n",
    "    if not os.path.exists(os.path.join(DATA_FOLDER, \"models\", \"word2vec.model\")):\n",
    "        if tokens_list is None:\n",
    "            raise ValueError(\"tokens_list must be provided to create the model\")\n",
    "        print(f\"Creating Word2Vec model with min_count={min_count}, window={window}, vector_size={vector_size}, workers={workers}, epochs={epochs}\")\n",
    "        w2v_model = Word2Vec(min_count=min_count,\n",
    "                            window=window,\n",
    "                            vector_size=vector_size,\n",
    "                            workers=workers)\n",
    "        \n",
    "        t = time()\n",
    "        w2v_model.build_vocab(tokens_list, progress_per=10000)\n",
    "        print('Time to build vocab: {} mins'.format(round((time() - t) / 60, 2)))\n",
    "        \n",
    "        t = time()\n",
    "        w2v_model.train(tokens_list, total_examples=w2v_model.corpus_count, epochs=epochs, report_delay=1)\n",
    "        print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))\n",
    "        \n",
    "        # Save the model\n",
    "        w2v_model.save(os.path.join(DATA_FOLDER, \"models\", model_name))\n",
    "        print(f\"Word2Vec model saved as {model_name}\")\n",
    "    else:\n",
    "        print(\"Word2Vec model already exists, loading it...\")\n",
    "        w2v_model = Word2Vec.load(os.path.join(DATA_FOLDER, \"models\", \"word2vec.model\"))\n",
    "        \n",
    "    return w2v_model\n",
    "\n",
    "# Create the Word2Vec model\n",
    "w2v_model = create_word2vec_model(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_df(file_name, w2v_model):\n",
    "#     \"\"\"\n",
    "#     Process the dataframe to get the vectors of the text\n",
    "    \n",
    "#     :param df_name: str, the name of the dataframe\n",
    "#     :return: pd.DataFrame, the dataframe with the vectors\n",
    "#     \"\"\"\n",
    "#     if os.path.exists(os.path.join(DATA_FOLDER, \"pickles\", file_name)):\n",
    "#         print(f\"Loading {file_name} vectors from the pickle file\")\n",
    "#         df = load_data(file_name)\n",
    "#     else:\n",
    "#         if \"json\" in file_name:\n",
    "#             df = pd.read_json(os.path.join(DATA_FOLDER, \"corpus\", file_name))\n",
    "#         else:\n",
    "#             df = pd.read_csv(os.path.join(DATA_FOLDER, file_name))\n",
    "#         print(f\"Processing {file_name} vectors\")\n",
    "#         if \"text\" in df.columns:\n",
    "#             df['tokenized'] = df.progress_apply(lambda row: tokenize(row['text'], lang=row['lang']), axis=1)\n",
    "#         elif \"query\" in df.columns:\n",
    "#             df['tokenized'] = df.progress_apply(lambda row: tokenize(row['query'], lang=row['lang']), axis=1)\n",
    "#         else:\n",
    "#             raise ValueError(\"The dataframe must contain a 'text' or 'query' column\")\n",
    "#         df['vectors'] = df['tokenized'].progress_apply(lambda x: vectorize(x, w2v_model))\n",
    "#         save_data(df, file_name.replace(\".csv\", \".pkl\"))\n",
    "    \n",
    "#     return df\n",
    "\n",
    "def process_df(file_name, w2v_model):\n",
    "    \"\"\"\n",
    "    Process the dataframe to get the vectors of the text\n",
    "    \n",
    "    :param df_name: str, the name of the dataframe\n",
    "    :return: pd.DataFrame, the dataframe with the vectors\n",
    "    \"\"\"\n",
    "    if os.path.exists(os.path.join(DATA_FOLDER, \"pickles\", file_name)):\n",
    "        print(f\"Loading {file_name} vectors from the pickle file\")\n",
    "        df = load_data(file_name)\n",
    "    else:\n",
    "        if \"json\" in file_name:\n",
    "            df = pd.read_json(os.path.join(DATA_FOLDER, \"corpus\", file_name))\n",
    "        else:\n",
    "            df = pd.read_csv(os.path.join(DATA_FOLDER, file_name))\n",
    "        print(f\"Processing {file_name} vectors\")\n",
    "        print(df.columns)\n",
    "        if \"text\" in df.columns:\n",
    "            df['vectors'] = df.progress_apply(lambda x: vectorize(x['text'],STOP_WORDS[x['lang']], w2v_model), axis=1)\n",
    "        elif \"query\" in df.columns:\n",
    "            df['vectors'] = df.progress_apply(lambda x: vectorize(x['query'],STOP_WORDS[x['lang']], w2v_model), axis=1)\n",
    "        else:\n",
    "            raise ValueError(\"The dataframe must contain a 'text' or 'query' column\")\n",
    "        \n",
    "        save_data(df, file_name.replace(\".csv\", \".pkl\"))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing test.csv vectors\n",
      "Index(['id', 'query_id', 'query', 'lang'], dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:00<00:00, 13380.35it/s]\n"
     ]
    }
   ],
   "source": [
    "df_queries_test = process_df(\"test.csv\", w2v_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "\n",
    "# Apply the tokenization\n",
    "corpus['tokenized'] = corpus.progress_apply(lambda row: tokenize(row['text'], lang=row['lang']), axis=1)\n",
    "tokens_list = corpus['tokenized'].tolist()\n",
    "corpus['vectors'] = corpus['tokenized'].progress_apply(lambda x: get_vectors(x, w2v_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_queries = pd.read_csv(os.path.join(DATA_FOLDER, \"train.csv\"))\n",
    "df_test_queries = pd.read_csv(os.path.join(DATA_FOLDER, \"test.csv\"))\n",
    "df_test_queries['tokenized'] = df_test_queries.progress_apply(lambda row: tokenize(row['query'], lang=row['lang']), axis=1)\n",
    "df_test_queries['vectors'] = df_test_queries['tokenized'].progress_apply(lambda x: get_vectors(x, w2v_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Calculate similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:31<00:00, 62.97it/s] \n"
     ]
    }
   ],
   "source": [
    "def rank_results(queries, df):\n",
    "    \"\"\"\n",
    "    Rank the results of the queries\n",
    "    \n",
    "    :param queries: pd.DataFrame, the queries\n",
    "    :param df: pd.DataFrame, the corpus\n",
    "    :param model: Word2Vec, the Word2Vec model\n",
    "\n",
    "    :return: pd.DataFrame, the ranked results\n",
    "    \"\"\"\n",
    "    # Extract vectors from queries and corpus\n",
    "    query_vectors = np.stack(queries['vectors'].values)\n",
    "    doc_vectors = np.stack(df['vectors'].values)\n",
    "\n",
    "    # Compute cosine similarities in one step for all query-document pairs\n",
    "    similarities = cosine_similarity(query_vectors, doc_vectors)\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for i, row in tqdm(enumerate(queries.iterrows()), total=len(queries)):\n",
    "        id = row[0]\n",
    "        # Get similarities for the current query and sort them\n",
    "        similarity_scores = similarities[i]\n",
    "        top10_indices = np.argsort(similarity_scores)[::-1][:10]\n",
    "        top10_docids = df.iloc[top10_indices]['docid'].tolist()\n",
    "\n",
    "        top10_results = {\n",
    "            \"id\": id,\n",
    "            \"docids\": top10_docids\n",
    "        }\n",
    "        results.append(top10_results)\n",
    "    \n",
    "    return results, similarities\n",
    "\n",
    "ranked_results, similarities = rank_results(df_queries_test, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save as csv\n",
    "ranked_results = pd.DataFrame(ranked_results)\n",
    "ranked_results.to_csv(os.path.join(DATA_FOLDER, \"ranked_results2.csv\"), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DIS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
