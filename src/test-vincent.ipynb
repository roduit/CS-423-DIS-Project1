{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# -*- authors : Vincent Roduit -*-\n",
    "# -*- date : 2024-09-30 -*-\n",
    "# -*- Last revision: 2024-09-30 by Vincent Roduit -*-\n",
    "# -*- python version : 3.9.19 -*-\n",
    "# -*- Description: Constants used in the code *-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> CS - 423: Distributed Information Systems </center>\n",
    "## <center> Ecole Polytechnique Fédérale de Lausanne </center>\n",
    "### <center>Project 1: Document Retrieval </center>\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "import pickle as pkl\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "import string\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from collections import Counter\n",
    "import swifter\n",
    "\n",
    "\n",
    "# automatically reload the module\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Declaring constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the data folder\n",
    "if os.path.exists(\"../data\"):\n",
    "    DATA_FOLDER = \"../data\"\n",
    "else:\n",
    "    # create the data folder\n",
    "    os.mkdir(\"../data\")\n",
    "\n",
    "CORPUS = os.path.join(DATA_FOLDER, \"corpus\", \"corpus.json\")\n",
    "CORPUS_PKL = os.path.join(DATA_FOLDER, \"pickles\", \"corpus.pkl\")\n",
    "\n",
    "LANG = {\n",
    "    \"en\": \"english\",\n",
    "    \"fr\": \"french\",\n",
    "    \"de\": \"german\",\n",
    "    \"es\": \"spanish\",\n",
    "    \"it\": \"italian\",\n",
    "    \"ko\": \"korean\",\n",
    "    \"ar\": \"arabic\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(data: any, file_name: str, folder: str = os.path.join(DATA_FOLDER, \"pickles\")):\n",
    "    \"\"\"\n",
    "    Save the data to a file\n",
    "    Args:\n",
    "\n",
    "    * data (any): the data to save\n",
    "\n",
    "    * file_name (str): the name of the file\n",
    "\n",
    "    * folder (str): the folder where to save the file\n",
    "    \"\"\"\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "\n",
    "    file_path = os.path.join(folder, file_name)\n",
    "\n",
    "    with open(file_path, 'wb') as handle:\n",
    "        pkl.dump(data, handle)\n",
    "\n",
    "def load_data(file_name: str, folder: str = os.path.join(DATA_FOLDER, \"pickles\")) -> any:\n",
    "    \"\"\"\n",
    "    Load the data from a file\n",
    "    Args:\n",
    "\n",
    "    * file_name (str): the name of the file\n",
    "\n",
    "    * folder (str): the folder where to save the file\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    * any: the data\n",
    "    \"\"\"\n",
    "    file_path = os.path.join(folder, file_name)\n",
    "\n",
    "    with open(file_path, 'rb') as handle:\n",
    "        data = pkl.load(handle)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the corpus from the pickle file\n"
     ]
    }
   ],
   "source": [
    "# load the corpus\n",
    "if os.path.exists(CORPUS_PKL):\n",
    "    print(\"Loading the corpus from the pickle file\")\n",
    "    corpus = load_data(\"corpus.pkl\")\n",
    "else:\n",
    "    print(\"Loading the corpus from the json file\")\n",
    "    corpus = pd.read_json(CORPUS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer() \n",
    "def tokenize(text, lang=\"english\"):\n",
    "    \"\"\"\n",
    "    It tokenizes and stems an input text.\n",
    "    \n",
    "    :param text: str, with the input text\n",
    "    :return: list, of the tokenized and stemmed tokens.\n",
    "    \"\"\"\n",
    "    text = \"\".join([ch for ch in text if ch not in string.punctuation])\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return [stemmer.stem(word.lower()) for word in tokens if word not in stopwords.words(lang)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [01:23<00:00,  5.96it/s]\n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas()\n",
    "\n",
    "# Apply the tokenization\n",
    "corpus['tokenized'] = corpus.progress_apply(lambda row: tokenize(row['text'], lang=LANG[row['lang']]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd859e65bf47476b8b77fea83496fece",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "corpus['tokenized'] = corpus.swifter.apply(lambda row: tokenize(row['text'], lang=LANG[row['lang']]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_list = corpus['tokenized'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Create the TF-IDF matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for the TF/IDF implementation\n",
    "\n",
    "def idf_values(vocabulary, documents):\n",
    "    \"\"\"\n",
    "    It computes IDF scores, storing idf values in a dictionary.\n",
    "    \n",
    "    :param vocabulary: list of str, with the unique tokens of the vocabulary.\n",
    "    :param documents: list of lists of str, with tokenized sentences.\n",
    "    :return: dict with the idf values for each vocabulary word.\n",
    "    \"\"\"\n",
    "    idf = {}\n",
    "    num_documents = len(documents)\n",
    "    \n",
    "    # Adding tqdm progress bar for the loop\n",
    "    for term in tqdm(vocabulary, desc=\"Calculating IDF values\"):\n",
    "        idf[term] = math.log(num_documents / sum([1 for d in documents if term in d]), math.e)\n",
    "    \n",
    "    return idf\n",
    "\n",
    "def vectorize(document, vocabulary, idf):\n",
    "    \"\"\"\n",
    "    It generates the vector for an input document (with normalization).\n",
    "    \n",
    "    :param document: list of str with the tokenized documents.\n",
    "    :param vocabulary: list of str, with the unique tokens of the vocabulary.\n",
    "    :param idf: dict with the idf values for each vocabulary word.\n",
    "    :return: list of floats\n",
    "    \"\"\"\n",
    "    vector = [0]*len(vocabulary)\n",
    "    counts = Counter(document)\n",
    "    max_count = counts.most_common(1)[0][1]\n",
    "    for i,term in enumerate(vocabulary):\n",
    "        vector[i] = idf[term] * counts[term] / max_count\n",
    "    return vector\n",
    "\n",
    "def cosine_similarity(v1,v2):\n",
    "    \"\"\"\n",
    "    It computes cosine similarity.\n",
    "    \n",
    "    :param v1: list of floats, with the vector of a document.\n",
    "    :param v2: list of floats, with the vector of a document.\n",
    "    :return: float\n",
    "    \"\"\"\n",
    "    \n",
    "    sumxx, sumxy, sumyy = 0, 0, 0\n",
    "    for i in range(len(v1)):\n",
    "        x = v1[i]; y = v2[i]\n",
    "        sumxx += x*x\n",
    "        sumyy += y*y\n",
    "        sumxy += x*y\n",
    "    if sumxy == 0:\n",
    "            result = 0\n",
    "    else:\n",
    "            result =  sumxy / math.sqrt(sumxx*sumyy)\n",
    "    return result\n",
    "\n",
    "def search_vec(query, topk, corpus, idf, vocabulary, document_vectors):\n",
    "    \"\"\"\n",
    "    It computes the search result (get the topk documents).\n",
    "    \n",
    "    :param query: str\n",
    "    :param topk: int\n",
    "    \"\"\"\n",
    "    q = query.split()\n",
    "    q = [stemmer.stem(w) for w in q]\n",
    "    query_vector = vectorize(q, vocabulary, idf)\n",
    "    scores = [[cosine_similarity(query_vector, document_vectors[d]), d] for d in range(len(corpus))]\n",
    "    scores.sort(key=lambda x: -x[0])\n",
    "    doc_ids = []\n",
    "    for i in range(topk):\n",
    "        doc_ids.append(scores[i][1])\n",
    "\n",
    "    return doc_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the vocabulary\n",
    "vocabulary = list(set([token for doc in corpus_list for token in doc]))\n",
    "vocabulary.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating IDF values:  17%|█▋        | 23056/138834 [02:39<13:20, 144.63it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m idf \u001b[38;5;241m=\u001b[39m idf_values(vocabulary, corpus_list)\n",
      "Cell \u001b[0;32mIn[27], line 16\u001b[0m, in \u001b[0;36midf_values\u001b[0;34m(vocabulary, documents)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Adding tqdm progress bar for the loop\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m term \u001b[38;5;129;01min\u001b[39;00m tqdm(vocabulary, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCalculating IDF values\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 16\u001b[0m     idf[term] \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39mlog(num_documents \u001b[38;5;241m/\u001b[39m \u001b[38;5;28msum\u001b[39m([\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m documents \u001b[38;5;28;01mif\u001b[39;00m term \u001b[38;5;129;01min\u001b[39;00m d]), math\u001b[38;5;241m.\u001b[39me)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m idf\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "idf = idf_values(vocabulary, corpus_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_vectors = [vectorize(s, vocabulary, idf) for s in corpus_list]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DIS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
